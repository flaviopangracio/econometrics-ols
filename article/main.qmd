---
title: "MÍNIMOS QUADRADOS ORDINÁRIOS: UMA APLICAÇÃO NA ANÁLISE DAS QUESTÕES INSTITUCIONAIS DE MUNICÍPIOS BRASILEIROS"
institution: "UNIVERSIDADE FEDERAL DE MINAS GERAIS"
advisor: "Ana Hermeto"
city: "Belo Horizonte"
state: "MG"
month: "Abril"
year: 2024
author:
    - name: "Flávio Hugo Pangracio Silva"
      orcid: 0000-0003-4045-101X
      email: "flaviopangracio@cedeplar.ufmg.br"
      affiliations:
        - name: "Cedeplar - UFMG"
          url: https://cedeplar.ufmg.br

    - name: "Guilherme Gomes Ferreira"
      orcid: 0009-0006-5032-8997
      email: "guilhermegf2019@cedeplar.ufmg.br"
      affiliations:
        - name: "Cedeplar - UFMG"
          url: https://cedeplar.ufmg.br
bibliography: referencies/ref.bib
csl: cite_styles/abnt.csl
highlight-style: atom-one
fig-cap-location: top
linkcolor: "black"
number-sections: true
lang: pt-BR
license: "This work is dedicated to the Public Domain"
execute:
  error: false
  warning: false
header-includes:
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\},fontsize=\footnotesize}
nocite: |
  @heiss, @wooldridge, @greene, @hansen
format:
    pdf:
        template-partials:
            - tex_files/_print-author.tex
            - tex_files/before-body.tex
            - tex_files/doc-class.tex
        mainfont: "Times New Roman"
---

\pagestyle{fancy} <!-- Show the page number --> \justify
\onehalfspacing

# INTRODUÇÃO

O presente trabalho se propõe a explorar de maneira detalhada o método de mínimos quadrados ordinários (MQO), apresentando uma aplicação na análise das questões institucionais presentes nos municípios brasileiros. Este método estatístico é amplamente utilizado na análise econômica, sendo fundamental para compreender as relações entre variáveis e realizar previsões.

A escolha desse enfoque se justifica pela relevância crescente do estudo das instituições no contexto municipal brasileiro, visto que as políticas públicas e a gestão eficiente dessas instituições desempenham um papel fundamental no desenvolvimento socioeconômico local. Nesse sentido, compreender como diferentes variáveis institucionais estão relacionadas entre si e como influenciam indicadores de crescimento e desenvolvimento municipal torna-se uma questão de interesse.

Por meio deste trabalho, pretendemos não apenas apresentar a aplicação prática do modelo de MQO, mas também fornecer uma base sólida de compreensão teórica, destacando os fundamentos matemáticos e estatísticos subjacentes a esse método. Para isso, organizaremos o conteúdo em várias seções, nas quais abordaremos desde os princípios básicos da regressão linear até aspectos mais avançados, passando pela discussão sobre a formulação teórica do modelo de MQO.

Inicialmente, abordaremos os principais conceitos e definições relacionados à regressão linear, discutindo os pressupostos e as limitações desse modelo estatístico. Posteriormente, dedicaremos atenção especial à formulação teórica do modelo de MQO, descrevendo o processo de estimativa dos parâmetros e apresentando as principais propriedades estatísticas dos estimadores obtidos por esse método. Além disso, discutiremos técnicas de diagnóstico e avaliação da qualidade do modelo, destacando a importância da interpretação correta dos resultados obtidos.

Por fim, demonstraremos a aplicação do modelo de MQO na análise das questões institucionais de municípios brasileiros, utilizando dados da REGIC 2018 para ilustrar o processo de formulação, estimação e interpretação do modelo. Espera-se que este trabalho contribua para ampliar o entendimento sobre o método de MQO e sua aplicação.

# O MODELO CLÁSSICO DE REGRESSÃO LINEAR

A priori, antes de adentrar em detalhes do estimador de MQO, é preciso explicar o modelo clássico de regressão linear, bem como suas hipóteses subjacentes. Nesse sentido, deve se salientar que o modelo clássico de regressão linear admite a forma simples e a forma múltipla. No modelo simples, também conhecido como modelo de regressão bivariada, temos apenas uma variável explicada e uma variável explicativa, além de um intercepto e dos resíduos do modelo. Um problema fundamental do modelo de regressão simples, no entanto, é a dificuldade de fazer uma análise parcial com apenas uma variável explicativa, ignorando todas outras variáveis que afetam a variável explicada, $Y$, e são não correlacionadas com a variável independente, $X$. É nesse sentido que existe o modelo de regressão linear múltipla, o qual permite explicar uma variável através de uma junção de mais variáveis independentes e não correlacionadas uma com a outra. Doravante, este trabalho focará no modelo de regressão linear múltipla, com a justificativa de que os pressupostos são análogos aos pressupostos do modelo simples e que com mais variáveis, o que só é permitido neste modelo, é possível fazer uma análise mais robusta. 

Nesta perspectiva, para a definição do modelo clássico de regressão linear, são necessárias algumas hipóteses:

## Linearidade do modelo

A primeira hipótese implica que o modelo deve ser linear nos parâmetros estimados. Disso decorre que as variáveis explicativas podem ser não lineares. Essa hipótese basicamente indica que a relação das variáveis independentes com o parâmetro estimado é linear (\ref{eqn:linear}), ou seja, uma variação marginal nas variáveis independentes resultará em uma variação constante na variável explicada.

```{=tex}
\begin{equation}
\label{eqn:linear}
y = x_1 \beta_1 + x_2 \beta_2 + \dots + x_k \beta_k + \varepsilon
\end{equation}
```

## Posto Completo

Essa hipótese é uma condição necessária do MCRL, haja vista que, se não satisfeita, é impossível estimar os paramêtros do modelo. Em termos matriciais, implica que a matriz das variáveis independentes deve ser não singular o que, por sua vez, exige que essas variáveis não sejam combinações lineares perfeitas umas das outras. Também é conhecida como condição de identificação.

## Exogeneidade

Tal condição garante que a média condicional do erro dadas as variáveis explicativas é igual a zero. Também conhecida como exogeneidade estrita, seu significado é de que as variáveis explicativas não possuem relação com o termo de perturbação (\ref{eqn:exo}). Além disso, é importante ressaltar que, como a média condicional do erro é zero, sua média incondicional também é zero, o que é garantido pela lei das expectativas iteradas (\ref{eqn:expectations}). Essa é uma forte implicação que garante que uma estimação pelo MCRL sempre acerta na média. Ademais, o MCRL garante a aleatoriedade dos resíduos, isto é, a média condicional do erro $i$, dado um erro $j$ qualquer é zero.

```{=tex}
\begin{equation}
\label{eqn:exo}
E[\mbfvarepsilon | \textbf{X}] = \begin{bmatrix} E[\varepsilon_1 | \textbf{X}] \\ E[\varepsilon_2 | \textbf{X}] \\ \vdots \\ E[\varepsilon_n | \textbf{X}] \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}
\end{equation}
```

```{=tex}
\begin{equation}
\label{eqn:expectations}
E[\varepsilon_i] = E_{\textbf{X}}[E[\varepsilon | \textbf{X}]] = E_{\textbf{X}}[0] = 0
\end{equation}
```

## Homocedasticidade e não autocorrelação residual

Essa quarta hipótese define que a variância condicional do erro é constante (\ref{eqn:var}) e que a covariância condicional dos erros é zero (\ref{eqn:cov}). A variância constante é conhecida como homocedasticidade, o que significa que para qualquer ponto da amostra, a variância sempre será a mesma. Quando isso não ocorre, dizemos que a variância é heterocedástica.

```{=tex}
\begin{equation}
\label{eqn:var}
Var[\varepsilon_i | \textbf{X}] = \sigma^2, \hspace{1cm} \forall i \in \{1, \dots, n\}.
\end{equation}
```

```{=tex}
\begin{equation}
\label{eqn:cov}
Cov[\varepsilon_i, \varepsilon_j | \textbf{X}] = 0, \hspace{1cm} \forall i \neq j.
\end{equation}
```

Já o fato da covariância condicional dor erros ser igual a zero define a não autocorrelação entre os termos de perturbação. Em termos matriciais, temos que a matriz de erros vezes a sua transposta é igual a matriz identidade vezes a variância dos resíduos (\ref{eqn:ee}). Vale ressaltar que isso não implica que as observações não são autocorrelacionadas.

```{=tex}
\begin{equation}
\label{eqn:ee}
E[\mbfvarepsilon \mbfvarepsilon' | \textbf{X}] =
\begin{bmatrix}
E[\varepsilon_1 \varepsilon_1 | \textbf{X}] && E[\varepsilon_1 \varepsilon_2 | \textbf{X}] && \cdots && E[\varepsilon_1 \varepsilon_n | \textbf{X}] \\
E[\varepsilon_2 \varepsilon_1 | \textbf{X}] && E[\varepsilon_2 \varepsilon_2 | \textbf{X}] && \cdots && E[\varepsilon_2 \varepsilon_n | \textbf{X}] \\
\vdots && \vdots && \ddots && \vdots \\
E[\varepsilon_n \varepsilon_1 | \textbf{X}] && E[\varepsilon_n \varepsilon_2 | \textbf{X}] && \cdots && E[\varepsilon_n \varepsilon_n | \textbf{X}]
\end{bmatrix} =
\begin{bmatrix}
\sigma^2 && 0 && \cdots && 0 \\
0 && \sigma^2 && \cdots && 0 \\
\vdots && \vdots && \ddots && \vdots \\
0 && 0 && \cdots && \sigma^2
\end{bmatrix}
\end{equation}
```

## Processo Gerador dos dados para a regressão

A quinta premissa se refere a não aleatoriedade do vetor de variáveis explicativas, em outras palavras, ele é não estocástico. Isso quer dizer que o vetor de variáveis explicativas é gerado exogenamente. No entanto, usualmente isso é de difícil aplicação, haja vista que o vetor $\textbf{X}$ tende a ser aleatório, tal qual o vetor $\textbf{Y}$. Desse modo, uma forma alternativa é assumir $\textbf{X}$ como um vetor aleatório e tratar da distribuição conjunta de $\textbf{X}$ e $\textbf{Y}$. Desse modo, essa premissa firma que $\textbf{X}$ pode ser fixo ou aleatório.

## Normalidade dos erros

Implica que os termos de perturbação são normalmente distribuídos, possuindo média zero e variância constante (\ref{eqn:normal}). Essa premissa é bastante razoável, haja vista que o teorema do limite central garante essa normalidade, pelo menos, assintoticamente. Todavia, essa suposição geralmente não é necessária para obter a maioria dos resultados em uma regressão linear. 

Finalizada esta parte, apresentou-se as premissas do MCRL, as quais servem como base para a construção de um modelo econométrico. O objetivo seguinte será descrever métodos de estimação de modelos, dentre eles, o famoso e amplamente utilizado, método de mínimos quadrados ordinários.

```{=tex}
\begin{equation}
\label{eqn:normal}
\varepsilon | \textbf{X} \sim N[\textbf{0}, \sigma^2 \textbf{I}]
\end{equation}
```

A figura \ref{fig:linearmodel} representa bem o Modelo Clássico de Regressão Linear, com os pressupostos definidos acima:

\fig{O Modelo Clássico de Regressão Linear}{fig:linearmodel}{images/linearModel.png}{Greene (2019)}

# REGRESSÃO POR MÍNIMOS QUADRADOS 

O método de mínimos quadrados ordinários consiste em minimizar a soma do quadrado dos resíduos, a fim de encontrar os parâmetros do modelo. O primeiro passo é distinguir entre as quantidades populacionais não observadas e os parâmetros amostrais. Em outras palavras, existem os parâmetros verdadeiros e os parâmetros calculados no modelo agem como uma estimativa desses parâmetros populacionais, desde que sejam satisfeitas as condições que tornem o MQO aplicável. Em termos matriciais, haja vista que estamos tratando de uma regressão linear múltipla, podemos escrever o modelo da seguinte forma: 

```{=tex}
\begin{equation}
\textbf{Y} = \textbf{X} \mbfbeta + \mbfvarepsilon
\end{equation}
```

```{=tex}
\begin{equation}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}_{n \times 1}
=
\begin{bmatrix}
1 && x_{12} && \cdots && x_{1k} \\
1 && x_{22} && \cdots && x_{2k} \\
\vdots && \vdots && \ddots && \vdots \\
1 && x_{n2} && \cdots && x_{nk}
\end{bmatrix}_{n \times k}
\cdot
\begin{bmatrix}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k \\
\end{bmatrix}_{k \times 1}
+
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_{n}
\end{bmatrix}_{n \times 1}
\end{equation}
```

```{=tex}
\begin{equation}
CPO: \hspace{1cm} \frac{\partial S(\mbfbeta)}{\partial \mbfbeta} = -2\textbf{X'Y} + 2\textbf{X'X} \mbfbeta = \textbf{0}
\end{equation}
```

```{=tex}
\begin{equation}
\implies \textbf{X'X} \mbfbeta = \textbf{X'Y}
\end{equation}
```

```{=tex}
\begin{equation}
\implies \mbfbeta = (\textbf{X'X})^{-1}\textbf{X'Y}
\end{equation}
```

Essa estimação nada mais é que as condições de primeira ordem do modelo. Nesse sentido, a partir dos valores estimados após encontrar os parâmetros amostrais, existem algumas relações importantes, sobretudo entre o termo de erro e os valores preditos da variável dependente: i) o MQO garante que a média dos resíduos é zero; ii) como não há covariância amostral entre o termo de erro e as variáveis independentes, não há covariância amostral entre os valores estimados e os resíduos; iii) os pontos médios das variáveis estão sempre sobre a reta de regressão. 

Além disso, sob a hipótese de homocedasticidade, tratada anteriormente no MCRL, existe um teorema, conhecido como teorema de Gauss-Markov, o qual garante que os estimadores de mínimos quadrados são os melhores estimadores não viesados da classe dos lineares. Todavia, esse teorema é muito restrito, haja vista as limitações impostas, como homocedasticidade e exogeneidade estrita, haja vista a ausência de viés. Nesse sentido, é mais vantajoso analisar as propriedades assintóticas dos estimadores, que tratam de convergência em probabilidade e flexibilizam mais o modelo estimado. 

Conforme Wooldrige (2010), para um estimador ser consistente, são necessárias duas premissas. A primeira implica que a covariância entre o resíduo e o vetor de variáveis explicativas seja igual a zero e essa é uma versão mais fraca da exogeneidade (\ref{eqn:cov}). Por outro lado, a segunda premissa diz que a multiplicação matricial de $\textbf{X}$ e sua transposta tem que ser igual a ordem de $\textbf{X}$ (\ref{eqn:posto}), implicando independência linear.

```{=tex}
\label{eqn:cov}
\begin{equation}
E[\textbf{X}'\mbfvarepsilon] = \textbf{0}
\end{equation}
```

```{=tex}
\label{eqn:posto}
\begin{equation}
E[\textbf{X'X}] = k
\end{equation}
```

## Ajuste do modelo

Se chamarmos de $\hat{\mbfbeta}$ o vetor de parâmetros estimados por MQO, podemos obter o vetor $\hat{\textbf{Y}}$ de valores estimados de $\textbf{Y}$. Todos esses valores estimados estarão necessariamente sobre a reta de regressão do MQO. A diferença entre $\hat{\textbf{Y}}$ e $\textbf{Y}$ será justamente o vetor de resíduos $\hat{\mbfvarepsilon}$. Ou seja, $\textbf{Y} - \hat{\textbf{Y}} = \hat{\mbfvarepsilon}$. Logo, se pensarmos em cada observação $i$, $\hat{\varepsilon_i}$ é a diferença entre $y_i$ e $\hat{y_i}$. Se $\hat{\varepsilon_i} > 0$, a reta subestima $y_i$; se $\hat{\varepsilon_i} < 0$ a reta superestima $y_i$; se $\hat{\varepsilon_i} = 0$, a reta passa exatamente sobre $y_i$.

Pela propriedade da equação (\ref{eqn:expectations}), temos que a média dos resíduos é zero. De forma equivalente, a média dos valores estimados é a mesma média amostral dos valores observados, $\overline{\hat{y}} = \overline{y}$. Dada essa característica, podemos definir medidas de perturbação em relação à média amostral $\hat{y}$:

```{=tex}
\label{eqn:sqt}
\begin{equation}
SQT = \sum^{n}_{i = 1} (y_i - \overline{y})^2
\end{equation}
```

```{=tex}
\label{eqn:sqe}
\begin{equation}
SQE = \sum^{n}_{i = 1} (\hat{y_i} - \overline{y})^2
\end{equation}
```

```{=tex}
\label{eqn:sqr}
\begin{equation}
SQR = \sum^{n}_{i = 1} \hat{\varepsilon_{i}}^2
\end{equation}
```

$SQT$ (Soma dos quadrados totais) mede a dispersão das $i$ observações ($y_i$), tendo a média amostral como centro. $SQE$ (Soma dos quadrados estimados) mede a dispersão das $i$ estimativas de $y$ ($\hat{y_i}$). $SQR$ (Soma dos quadrados dos resídiuos) mede a variação dos erros estimados ($\hat{\varepsilon_i}$). A variação total em $y$ pode ser sempre espressada como a soma da variação explicada e da variação não explicada (ou dos resíduos):

```{=tex}
\label{eqn:soma}
\begin{equation}
SQT = SQE + SQR
\end{equation}
```

### Grau de Ajuste

As definições acima nos ajudarão à definir uma medida de ajuste do modelo, ou seja, dizer quão bem nossas variáveis independentes ($\textbf{X}$) explicam as observações da variável dependente ($\textbf{Y}$).

Se assumirmos que $SQT \neq 0$, podemos definir uma medida conhecida como $R^2$ (coeficiente de determinação), que basicamente é uma proporção entre a variação explicada (SQE) e a variação total (SQT). Em outras palavras, é uma medida que diz quanto da variação total em $y$ foi explicada pelas variáveis $x$.

```{=tex}
\label{eqn:rquadrado}
\begin{equation}
R^2 = \frac{SQE}{SQT} = 1 - \frac{SQR}{SQT}
\end{equation}
```

$$R^2 \in [0, 1]$$

Como $SQE \leq SQT$, $R^2$ sempre estará entre $0$ e $1$. Graficamente falando, quanto mais próximo de $1$, mais próximas as observações estarão da reta de regressão.

# APLICAÇÃO

Para aplicar as questões tratadas nas últimas seções, vamos estimar um modelo linear por MQO, utilizando a linguagem R e os dados da REGIC 2018 para municípios de Minas Gerais. 

Carregando pacotes:
```{r}
library(readxl)
library(dplyr)
library(ggplot2)
library(stargazer)
library(geobr)
library(ggspatial)
library(ggrepel)
library(scatterplot3d)
```

Carregando a base da REGIC 2018 e ajustando as variáveis de interesse:

```{r}
df_regic <- readxl::read_xlsx(
  path = "data/REGIC2018 Cidades v2.xlsx",
  sheet="Base de dados por Cidades"
) |>
  dplyr::filter(
    UF == "MG"
  ) |>
  dplyr::select(
    "COD_CIDADE",
    "NOME_CIDADE",
    "VAR01",
    "VAR03",
    "VAR23",
    "VAR29",
    "VAR85",
    "VAR89"
  ) |>
  dplyr::rename(
    "populacao" = "VAR01",
    "pib" = "VAR03",
    "cige" = "VAR23",
    "cgp" = "VAR29"
  ) |>
  dplyr::mutate(
    "populacao" = as.numeric(populacao),
    "pib" = as.numeric(pib),
    "cige" = as.numeric(cige),
    "cgp" = as.numeric(cgp),
    "banco_publico" = ifelse(VAR85 | VAR89, 1, 0),
    "log_cige" = ifelse(as.numeric(cige) < 1, 0, log(as.numeric(cige))),
    "log_cgp" = ifelse(as.numeric(cgp) < 1, 0, log(as.numeric(cgp)))
  )

df_regic <- na.omit(df_regic)

df_regic$pib_pc <- df_regic$pib / df_regic$populacao
```

```{r echo=FALSE}

palette1 <- c("#264653", "#2a9d8f", "#e9c46a", "#f4a261","#e76f51")

municipios <- geobr::read_municipality(code_muni = 31, showProgress = FALSE)

municipios <- municipios |> dplyr::left_join(
  df_regic,
  by = c("code_muni" = "COD_CIDADE")
)

pib_map <- ggplot2::ggplot(municipios) +
  ggplot2::geom_sf(mapping = aes(fill = pib_pc), lwd = 0, colour = NA) +
  ggplot2::scale_fill_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "PIB per capita",
    na.value = "grey80",
  ) +
  theme_void() +
  theme(
    text = element_text(face = "plain"),
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12, face = "plain"),
  ) +
  annotation_scale(location = "bl", line_width = 0.4) +
  annotation_north_arrow(
    location = "br",
    style = north_arrow_fancy_orienteering(),
    height = unit(1, "cm"),
    width = unit(1, "cm")
  )

ggsave("exports/pibmap.pdf", plot = pib_map, width = 5000, height = 5000, units = "px", dpi = 320)

cige_map <- ggplot2::ggplot(municipios) +
  ggplot2::geom_sf(mapping = aes(fill = cige), lwd = 0, colour = NA) +
  ggplot2::scale_fill_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "CIGE",
    na.value = "grey80",
  ) +
  theme_void() +
  theme(
    text = element_text(face = "plain"),
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12, face = "plain"),
  ) +
  annotation_scale(location = "bl", line_width = 0.4) +
  annotation_north_arrow(
    location = "br",
    style = north_arrow_fancy_orienteering(),
    height = unit(1, "cm"),
    width = unit(1, "cm")
  )

ggsave("exports/cigemap.pdf", plot = cige_map, width = 5000, height = 5000, units = "px", dpi = 320)

cgp_map <- ggplot2::ggplot(municipios) +
  ggplot2::geom_sf(mapping = aes(fill = cgp), lwd = 0, colour = NA) +
  ggplot2::scale_fill_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "CGP",
    na.value = "grey80",
  ) +
  theme_void() +
  theme(
    text = element_text(face = "plain"),
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12, face = "plain"),
  ) +
  annotation_scale(location = "bl", line_width = 0.4) +
  annotation_north_arrow(
    location = "br",
    style = north_arrow_fancy_orienteering(),
    height = unit(1, "cm"),
    width = unit(1, "cm")
  )

ggsave("exports/cgpmap.pdf", plot = cgp_map, width = 5000, height = 5000, units = "px", dpi = 320)
```

Foi necessário excluir os registros que tinham alguma variável de interesse NULA (NA). Optou-se por não substituir os *NA* por valores arbitrários, como zero ou qualquer outro valor, pois causaria um grande viés na regressão, visto que se tratam municípios que não tiverem essas variáveis mensuradas.

## Análise Descritiva

Nesta seção, uma análise descritiva abrangente das variáveis em questão será conduzida, visando fornecer uma compreensão profunda de seus padrões, distribuições e relações. Esta abordagem permite não apenas a caracterização detalhada de cada variável individualmente, mas também a identificação de tendências e padrões globais dentro do conjunto de dados.

A Tabela @tbl-sum-vars contém as principais estatísticas descritivas das variáveis analisadas (PIB per capita, Centralidade da Gestão Pública (CGP) e Coeficiente de Intensidade da Gestão Empresarial (CIGE)) para as 168 observações (municípios) restantes na base:
```{r, results='asis', echo=FALSE}
#| label: tbl-sum-vars
#| tbl-cap: Estatísticas descritivas das variáveis
summary_vars <- data.frame(as.list(summary(df_regic$cige))) |>
  rbind(data.frame(as.list(summary(df_regic$cgp)))) |>
  rbind(data.frame(as.list(summary(df_regic$pib_pc))))

summary_vars <- cbind(c("CIGE", "CGP", "PIBpc"), summary_vars)

names(summary_vars) <- c("Variável", names(summary(df_regic$cige)))

stargazer::stargazer(
  summary_vars,
  header=FALSE,
  type='latex',
  summary = FALSE,
  rownames = FALSE,
  style = "aer",
  notes = "Fonte: Elaboração própria.",
  digits = 2,
  font.size = "small",
  no.space = TRUE
)
```

Abaixo, plotamos os histogramas dessas variáveis:
```{r, echo=FALSE}
#| fig-cap: Histograma do PIB per capita dos municípios de Minas Gerais (2018)

ggplot2::ggplot(df_regic, aes(x = pib_pc)) +
  ggplot2::geom_histogram(
    aes(y = ..density..),
    fill = "#00b4ef",
    color = "#ffffff",
  ) +
  ggplot2::geom_density(color="#bf80ff", size=0.5) +
  ggplot2::ylim(0, 0.075) +
  ggplot2::labs(
    x = "PIB per capita",
    y = "Densidade",
  ) +
  ggplot2::theme_classic() +
  ggplot2::theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(size = 0.3),
    title = element_text(size = 8, colour = "#222222"),
    plot.title = element_text(hjust = 0.5)
  )
```

```{r, echo=FALSE}
#| fig-cap: Histograma do Coeficiente de Intensidade da Gestão Empresarial (CIGE) dos municípios de Minas Gerais (2018)
ggplot2::ggplot(df_regic, aes(x = cige)) +
  ggplot2::geom_histogram(
    aes(y = ..density..),
    fill = "#00b4ef",
    color = "#ffffff",
  ) +
  ggplot2::geom_density(color="#bf80ff", size=0.5) +
  ggplot2::labs(
    x = "CIGE",
    y = "Densidade",
  ) +
  ggplot2::theme_classic() +
  ggplot2::theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(size = 0.3),
    title = element_text(size = 8, colour = "#222222"),
    plot.title = element_text(hjust = 0.5)
  )
```

```{r, echo=FALSE}
#| fig-cap: Histograma da Centralidade da Gestão Pública (CGP) dos municípios de Minas Gerais (2018)

ggplot2::ggplot(df_regic, aes(x = cgp)) +
  ggplot2::geom_histogram(
    aes(y = ..density..),
    fill = "#00b4ef",
    color = "#ffffff",
  ) +
  ggplot2::geom_density(color="#bf80ff", size=0.5) +
  ggplot2::labs(
    x = "CGP",
    y = "Densidade",
  ) +
  ggplot2::theme_classic() +
  ggplot2::theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(size = 0.3),
    title = element_text(size = 8, colour = "#222222"),
    plot.title = element_text(hjust = 0.5)
  )
```

Podemos ver a correlação entre as variáveis por meio de um mapa de calor obtido da matriz de correlação:
```{r, results='asis'}
#| label: fig-cor-heatmap
#| fig-cap: "Mapa de calor da matriz de correlação."

# Matriz com as variáveis [Y|X]
matrix <- df_regic |>
  dplyr::select("pib_pc", "cige", "cgp") |>
  as.matrix.data.frame()


matrix |> cor() |> heatmap()
```

Outra medida importante é a de covariância entre as variáveis, descrita na matriz de covariância abaixo (variância na diagonal principal):

```{r}
matrix |> cov() |> round(2)
```

Abaixo plotamos os *boxplots* das variáveis utilizadas:
```{r}
#| label: fig-box-cgp
#| fig-cap: "Boxplot da Centralidade da Gestão Pública (CGP) dos municípios de Minas Gerais (2018)"
boxplot(df_regic$cgp)
```

```{r}
#| label: fig-box-cige
#| fig-cap: "Boxplot do Coeficiente de Intensidade da Gestão Empresarial (CIGE) dos municípios de Minas Gerais (2018)"
boxplot(df_regic$cige)
```

```{r}
#| label: fig-box-pib
#| fig-cap: "Boxplot do PIB per capita dos municípios de Minas Gerais (2018)"
boxplot(df_regic$cige)
```

A visualização geográfica das variáveis do modelo (para o caso de municípios) é essencial para compreender, com clareza, onde os processos estão acontecendo:

\fig{PIB per capita dos municípios de Minas Gerais - REGIC 2018}{fig:pibpc}{exports/pibmap.pdf}{Elaboração própria.}

\fig{CIGE dos municípios de Minas Gerais - REGIC 2018}{fig:cige}{exports/cigemap.pdf}{Elaboração própria.}

\fig{CGP dos municípios de Minas Gerais - REGIC 2018}{fig:cgp}{exports/cgpmap.pdf}{Elaboração própria.}

Por fim, podemos plotar alguns gráficos de dispersão para esboçar a relação entre as variáveis do modelo:

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-cige-scatter
#| fig-cap: "PIB per capita x CIGE (Municípios de Minas Gerais - REGIC 2018)"

muni <- municipios[order(-municipios$pib_pc), ]
top3 <- muni$name_muni[1:3]

muni <- municipios[order(municipios$pib_pc), ]
low3 <- muni$name_muni[1:3]

ggplot2::ggplot(municipios,  aes(x = log(cige), y = log(pib_pc), label = name_muni)) +
  ggplot2::geom_point(mapping = aes(colour = pib_pc)) +
  ggplot2::scale_color_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "PIB per capita",
  ) +
  ggplot2::labs(
    x = "Log do CIGE",
    y = "Log do PIB per capita",
  ) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-cgp-scatter
#| fig-cap: "PIB per capita x CGP (Municípios de Minas Gerais - REGIC 2018)"

muni <- municipios[order(-municipios$pib_pc), ]
top3 <- muni$name_muni[1:3]

muni <- municipios[order(municipios$pib_pc), ]
low3 <- muni$name_muni[1:3]

ggplot2::ggplot(municipios,  aes(x = log(cgp), y = log(pib_pc), label = name_muni)) +
  ggplot2::geom_point(mapping = aes(colour = pib_pc)) +
  ggplot2::scale_color_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "PIB per capita",
  ) +
  ggplot2::labs(
    x = "Log do CGP",
    y = "Log do PIB per capita",
  ) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

## Análise de Regressão

Com as mesmas operações definidas na seção sobre Regressão Por Mínimos Quadrados, vamos estimar um modelo linear que tenta explicar o $ln(PIBpc)$ (logaritmo natural do PIB per capita) de municípios mineiros, com variáveis que medem intensidade de gestão empresarial (CIGE) e nível de centralidade da gestão pública (CGP).

```{r}
# Regressão simples
covxy <- cov(df_regic$log_cige, log(df_regic$pib_pc))
varx <- var(df_regic$log_cige)
mediay <- mean(log(df_regic$pib_pc))
mediax <- mean(df_regic$log_cige)
b1 <- covxy/varx

b0 <- mediay - b1*mediax

print(paste0("Intercepto: ", round(b0, 2)))
print(paste0("Coeficiente estimado: ", round(b1, 2)))
```

Com esses coeficientes já é possível traçar uma reta de regressão no gráfico:

```{r, echo=FALSE}
ggplot2::ggplot(municipios,  aes(x = log(cige), y = log(pib_pc), label = name_muni)) +
  ggplot2::geom_point(mapping = aes(colour = pib_pc)) +
  ggplot2::geom_abline(intercept = b0, slope = b1, color = "red") +
  ggplot2::scale_color_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "PIB per capita",
  ) +
  ggplot2::labs(
    x = "Log do CIGE",
    y = "Log do PIB per capita",
  ) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

Para verificar o ajuste do modelo, podemos calcular o $R^2$, dado que $R^2 = Var(\hat{y})/Var(y)$:

```{r}
# Cálculo do R-quadrado
y <- log(df_regic$pib_pc)
x <- log(df_regic$cige)
y.hat <- b0 + b1*x
r2 <- var(y.hat)/var(y)
r2
```

Esse resultado indica que cerca de $16,10\%$ da variação do log do PIB per capita dos municípios da amostra é explicado pelo log do Coeficiente de Interação da Gestão Empresarial (CIGE) desses municípios.

## Análise de Regressão Múltipla

Vamos acrescentar mais uma variável no modelo anterior e estimar novamente os parâmetros por meio de álgebra matricial:

```{r}
# Número de observações da amostra
n <- nrow(df_regic)

# Número de variáveis independentes
k <- 2

# Matrix de variáveis independentes
X <- matrix(1, nrow = n, ncol = 3) # Coluna de 1 (intercepto)
X[ ,2] <- log(df_regic$cige) # CIGE
X[ ,3] <- log(df_regic$cgp) # CGP

# Vetor de variável observada
Y <- as.matrix(log(df_regic$pib_pc))

# Vetor de parâmetros estimados
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y

# Vetor de resíduos estimados
uhat <- y - X %*% bhat

# Variância dos resíduos
sigma2hat <- as.numeric (t(uhat) %*% uhat / (nrow(df_regic)-k-1))

# Matriz var-cov dos coeficientes
varbetahat <- sigma2hat * solve(t(X) %*% X)
erropadraobeta <- sqrt (diag(varbetahat))

bhat
sigma2hat
varbetahat
erropadraobeta
```

Com apenas 2 variáveis independentes ainda é possível ver graficamente essa regressão, e agora, ao invés de uma reta de regressão, temos um plano de regressão:

```{r, echo=FALSE, warning=FALSE}
mqo <- lm(log(pib_pc) ~ log(cige) + log(cgp), df_regic)
grafico <- scatterplot3d(
  log(df_regic$cige), log(df_regic$cgp), log(df_regic$pib_pc),
  main = "Regressão Múltipla",
  xlab = "CIGE",
  ylab = "CGP",
  zlab = "PIB per capita",
  color = "blue"
)

y.hat <- X %*% bhat

grafico$plane3d(mqo, col = "red", alpha = 0.5, draw_polygon = T, draw_lines = T)
```

Como fizemos para a regressão simples, podemos calcular o ajuste $R^2$ para o modelo de regressão múltipla:

```{r}
r2 <- var(y.hat)/var(y)
r2
```

Esse resultado indica que cerca de $21,55\%$ da variação do log do PIB per capita dos
municípios da amostra é explicada pela combinação linear entre o log do Coeficiente de Interação da Gestão Empresarial(CIGE) e o log do nível de Centralidade da Gestão Pública (CGP) desses municípios.

\newpage

# REFERÊNCIAS

\singlespacing

::: {#refs}
:::