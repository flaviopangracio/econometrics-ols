---
title: "MÍNIMOS QUADRADOS ORDINÁRIOS: UMA APLICAÇÃO NA ANÁLISE DAS QUESTÕES INSTITUCIONAIS DE MUNICÍPIOS BRASILEIROS"
institution: "UNIVERSIDADE FEDERAL DE MINAS GERAIS"
advisor: "Ana Hermeto"
city: "Belo Horizonte"
state: "MG"
month: "Abril"
year: 2024
author:
    - name: "Flávio Hugo Pangracio Silva"
      orcid: 0000-0003-4045-101X
      email: "flaviopangracio@cedeplar.ufmg.br"
      affiliations:
        - name: "Cedeplar - UFMG"
          url: https://cedeplar.ufmg.br

    - name: "Guilherme Gomes Ferreira"
      orcid: 0009-0006-5032-8997
      email: "guilhermegf2019@cedeplar.ufmg.br"
      affiliations:
        - name: "Cedeplar - UFMG"
          url: https://cedeplar.ufmg.br

    - name: "Rayner Filomeno"
      orcid:
      email: "rayner.filomeno@gmail.com"
      affiliations:
        - name: "Cedeplar - UFMG"
          url: https://cedeplar.ufmg.br

bibliography: referencies/ref.bib
csl: cite_styles/abnt.csl
highlight-style: atom-one
fig-cap-location: top
linkcolor: "black"
number-sections: true
lang: pt-BR
license: "This work is dedicated to the Public Domain"
execute:
  error: false
  warning: false
header-includes:
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\},fontsize=\footnotesize}
nocite: |
  @heiss, @wooldridge, @greene, @hansen
format:
    pdf:
        template-partials:
            - tex_files/_print-author.tex
            - tex_files/before-body.tex
            - tex_files/doc-class.tex
        mainfont: "Times New Roman"
---

\pagestyle{fancy} <!-- Show the page number --> \justify
\onehalfspacing

# INTRODUÇÃO

O presente trabalho se propõe a explorar de maneira detalhada o método de mínimos quadrados ordinários (MQO), apresentando uma aplicação na análise das questões institucionais presentes nos municípios brasileiros. Este método estatístico é amplamente utilizado na análise econômica, sendo fundamental para compreender as relações entre variáveis e realizar previsões.

A escolha desse enfoque se justifica pela relevância crescente do estudo das instituições no contexto municipal brasileiro, visto que as políticas públicas e a gestão eficiente dessas instituições desempenham um papel fundamental no desenvolvimento socioeconômico local. Nesse sentido, compreender como diferentes variáveis institucionais estão relacionadas entre si e como influenciam indicadores de crescimento e desenvolvimento municipal torna-se uma questão de interesse.

Por meio deste trabalho, pretendemos não apenas apresentar a aplicação prática do modelo de MQO, mas também fornecer uma base sólida de compreensão teórica, destacando os fundamentos matemáticos e estatísticos subjacentes a esse método. Para isso, organizaremos o conteúdo em várias seções, nas quais abordaremos desde os princípios básicos da regressão linear até aspectos mais avançados, passando pela discussão sobre a formulação teórica do modelo de MQO.

Inicialmente, abordaremos os principais conceitos e definições relacionados à regressão linear, discutindo os pressupostos e as limitações desse modelo estatístico. Posteriormente, dedicaremos atenção especial à formulação teórica do modelo de MQO, descrevendo o processo de estimativa dos parâmetros e apresentando as principais propriedades estatísticas dos estimadores obtidos por esse método. Além disso, discutiremos técnicas de diagnóstico e avaliação da qualidade do modelo, destacando a importância da interpretação correta dos resultados obtidos.

Por fim, demonstraremos a aplicação do modelo de MQO na análise das questões institucionais de municípios brasileiros, utilizando dados da REGIC 2018 para ilustrar o processo de formulação, estimação e interpretação do modelo. Espera-se que este trabalho contribua para ampliar o entendimento sobre o método de MQO e sua aplicação.

# O MODELO CLÁSSICO DE REGRESSÃO LINEAR

A priori, antes de adentrar em detalhes do estimador de MQO, é preciso explicar o modelo clássico de regressão linear, bem como suas hipóteses subjacentes. Nesse sentido, deve se salientar que o modelo clássico de regressão linear admite a forma simples e a forma múltipla. No modelo simples, também conhecido como modelo de regressão bivariada, temos apenas uma variável explicada e uma variável explicativa, além de um intercepto e dos resíduos do modelo. Um problema fundamental do modelo de regressão simples, no entanto, é a dificuldade de fazer uma análise parcial com apenas uma variável explicativa, ignorando todas outras variáveis que afetam a variável explicada, $Y$, e são não correlacionadas com a variável independente, $X$. É nesse sentido que existe o modelo de regressão linear múltipla, o qual permite explicar uma variável através de uma junção de mais variáveis independentes e não correlacionadas uma com a outra. Doravante, este trabalho focará no modelo de regressão linear múltipla, com a justificativa de que os pressupostos são análogos aos pressupostos do modelo simples e que com mais variáveis, o que só é permitido neste modelo, é possível fazer uma análise mais robusta.

Nesta perspectiva, para a definição do modelo clássico de regressão linear, são necessárias algumas hipóteses:

*Linearidade do modelo*

A primeira hipótese implica que o modelo deve ser linear nos parâmetros estimados. Disso decorre que as variáveis explicativas podem ser não lineares. Essa hipótese basicamente indica que a relação das variáveis independentes com o parâmetro estimado é linear (\ref{eqn:linear}), ou seja, uma variação marginal nas variáveis independentes resultará em uma variação constante na variável explicada.

```{=tex}
\begin{equation}
\label{eqn:linear}
y = x_1 \beta_1 + x_2 \beta_2 + \dots + x_k \beta_k + \varepsilon
\end{equation}
```

*Posto Completo*

Essa hipótese é uma condição necessária do MCRL, haja vista que, se não satisfeita, é impossível estimar os paramêtros do modelo. Em termos matriciais, implica que a matriz das variáveis independentes deve ser não singular o que, por sua vez, exige que essas variáveis não sejam combinações lineares perfeitas umas das outras. Também é conhecida como condição de identificação.

*Exogeneidade*

Tal condição garante que a média condicional do erro dadas as variáveis explicativas é igual a zero. Também conhecida como exogeneidade estrita, seu significado é de que as variáveis explicativas não possuem relação com o termo de perturbação (\ref{eqn:exo}). Além disso, é importante ressaltar que, como a média condicional do erro é zero, sua média incondicional também é zero, o que é garantido pela lei das expectativas iteradas (\ref{eqn:expectations}). Essa é uma forte implicação que garante que uma estimação pelo MCRL sempre acerta na média. Ademais, o MCRL garante a aleatoriedade dos resíduos, isto é, a média condicional do erro $i$, dado um erro $j$ qualquer é zero.

```{=tex}
\begin{equation}
\label{eqn:exo}
E[\mbfvarepsilon | \textbf{X}] = \begin{bmatrix} E[\varepsilon_1 | \textbf{X}] \\ E[\varepsilon_2 | \textbf{X}] \\ \vdots \\ E[\varepsilon_n | \textbf{X}] \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}
\end{equation}
```

```{=tex}
\begin{equation}
\label{eqn:expectations}
E[\varepsilon_i] = E_{\textbf{X}}[E[\varepsilon | \textbf{X}]] = E_{\textbf{X}}[0] = 0
\end{equation}
```

*Homocedasticidade e não autocorrelação residual*

Essa quarta hipótese define que a variância condicional do erro é constante (\ref{eqn:var}) e que a covariância condicional dos erros é zero (\ref{eqn:cov}). A variância constante é conhecida como homocedasticidade, o que significa que para qualquer ponto da amostra, a variância sempre será a mesma. Quando isso não ocorre, dizemos que a variância é heterocedástica.

```{=tex}
\begin{equation}
\label{eqn:var}
Var[\varepsilon_i | \textbf{X}] = \sigma^2, \hspace{1cm} \forall i \in \{1, \dots, n\}.
\end{equation}
```

```{=tex}
\begin{equation}
\label{eqn:cov}
Cov[\varepsilon_i, \varepsilon_j | \textbf{X}] = 0, \hspace{1cm} \forall i \neq j.
\end{equation}
```

Já o fato da covariância condicional dor erros ser igual a zero define a não autocorrelação entre os termos de perturbação. Em termos matriciais, temos que a matriz de erros vezes a sua transposta é igual a matriz identidade vezes a variância dos resíduos (\ref{eqn:ee}). Vale ressaltar que isso não implica que as observações não são autocorrelacionadas.

```{=tex}
\begin{equation}
\label{eqn:ee}
E[\mbfvarepsilon \mbfvarepsilon' | \textbf{X}] =
\begin{bmatrix}
E[\varepsilon_1 \varepsilon_1 | \textbf{X}] && E[\varepsilon_1 \varepsilon_2 | \textbf{X}] && \cdots && E[\varepsilon_1 \varepsilon_n | \textbf{X}] \\
E[\varepsilon_2 \varepsilon_1 | \textbf{X}] && E[\varepsilon_2 \varepsilon_2 | \textbf{X}] && \cdots && E[\varepsilon_2 \varepsilon_n | \textbf{X}] \\
\vdots && \vdots && \ddots && \vdots \\
E[\varepsilon_n \varepsilon_1 | \textbf{X}] && E[\varepsilon_n \varepsilon_2 | \textbf{X}] && \cdots && E[\varepsilon_n \varepsilon_n | \textbf{X}]
\end{bmatrix} =
\begin{bmatrix}
\sigma^2 && 0 && \cdots && 0 \\
0 && \sigma^2 && \cdots && 0 \\
\vdots && \vdots && \ddots && \vdots \\
0 && 0 && \cdots && \sigma^2
\end{bmatrix}
\end{equation}
```

*Processo Gerador dos dados para a regressão*

A quinta premissa se refere a não aleatoriedade do vetor de variáveis explicativas, em outras palavras, ele é não estocástico. Isso quer dizer que o vetor de variáveis explicativas é gerado exogenamente. No entanto, usualmente isso é de difícil aplicação, haja vista que o vetor $\textbf{X}$ tende a ser aleatório, tal qual o vetor $\textbf{Y}$. Desse modo, uma forma alternativa é assumir $\textbf{X}$ como um vetor aleatório e tratar da distribuição conjunta de $\textbf{X}$ e $\textbf{Y}$. Desse modo, essa premissa firma que $\textbf{X}$ pode ser fixo ou aleatório.

*Normalidade dos erros*

Implica que os termos de perturbação são normalmente distribuídos, possuindo média zero e variância constante (\ref{eqn:normal}). Essa premissa é bastante razoável, haja vista que o teorema do limite central garante essa normalidade, pelo menos, assintoticamente. Todavia, essa suposição geralmente não é necessária para obter a maioria dos resultados em uma regressão linear.

Finalizada esta parte, apresentou-se as premissas do MCRL, as quais servem como base para a construção de um modelo econométrico. O objetivo seguinte será descrever métodos de estimação de modelos, dentre eles, o famoso e amplamente utilizado, método de mínimos quadrados ordinários.

```{=tex}
\begin{equation}
\label{eqn:normal}
\varepsilon | \textbf{X} \sim N[\textbf{0}, \sigma^2 \textbf{I}]
\end{equation}
```

A figura \ref{fig:linearmodel} representa bem o Modelo Clássico de Regressão Linear, com os pressupostos definidos acima:

\fig{O Modelo Clássico de Regressão Linear}{fig:linearmodel}{images/linearModel.png}{Greene (2019)}

# REGRESSÃO POR MÍNIMOS QUADRADOS

O método de mínimos quadrados ordinários consiste em minimizar a soma do quadrado dos resíduos, a fim de encontrar os parâmetros do modelo. O primeiro passo é distinguir entre as quantidades populacionais não observadas e os parâmetros amostrais. Em outras palavras, existem os parâmetros verdadeiros e os parâmetros calculados no modelo agem como uma estimativa desses parâmetros populacionais, desde que sejam satisfeitas as condições que tornem o MQO aplicável. Em termos matriciais, haja vista que estamos tratando de uma regressão linear múltipla, podemos escrever o modelo da seguinte forma:

```{=tex}
\begin{equation}
\textbf{Y} = \textbf{X} \mbfbeta + \mbfvarepsilon
\end{equation}
```

```{=tex}
\begin{equation}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}_{n \times 1}
=
\begin{bmatrix}
1 && x_{12} && \cdots && x_{1k} \\
1 && x_{22} && \cdots && x_{2k} \\
\vdots && \vdots && \ddots && \vdots \\
1 && x_{n2} && \cdots && x_{nk}
\end{bmatrix}_{n \times k}
\cdot
\begin{bmatrix}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k \\
\end{bmatrix}_{k \times 1}
+
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_{n}
\end{bmatrix}_{n \times 1}
\end{equation}
```

```{=tex}
\begin{equation}
CPO: \hspace{1cm} \frac{\partial S(\mbfbeta)}{\partial \mbfbeta} = -2\textbf{X'Y} + 2\textbf{X'X} \mbfbeta = \textbf{0}
\end{equation}
```

```{=tex}
\begin{equation}
\implies \textbf{X'X} \mbfbeta = \textbf{X'Y}
\end{equation}
```

```{=tex}
\begin{equation}
\implies \mbfbeta = (\textbf{X'X})^{-1}\textbf{X'Y}
\end{equation}
```

Essa estimação nada mais é que as condições de primeira ordem do modelo. Nesse sentido, a partir dos valores estimados após encontrar os parâmetros amostrais, existem algumas relações importantes, sobretudo entre o termo de erro e os valores preditos da variável dependente: i) o MQO garante que a média dos resíduos é zero; ii) como não há covariância amostral entre o termo de erro e as variáveis independentes, não há covariância amostral entre os valores estimados e os resíduos; iii) os pontos médios das variáveis estão sempre sobre a reta de regressão.

Além disso, sob a hipótese de homocedasticidade, tratada anteriormente no MCRL, existe um teorema, conhecido como teorema de Gauss-Markov, o qual garante que os estimadores de mínimos quadrados são os melhores estimadores não viesados da classe dos lineares. Todavia, esse teorema é muito restrito, haja vista as limitações impostas, como homocedasticidade e exogeneidade estrita, haja vista a ausência de viés. Nesse sentido, é mais vantajoso analisar as propriedades assintóticas dos estimadores, que tratam de convergência em probabilidade e flexibilizam mais o modelo estimado.

Conforme Wooldrige (2010), para um estimador ser consistente, são necessárias duas premissas. A primeira implica que a covariância entre o resíduo e o vetor de variáveis explicativas seja igual a zero e essa é uma versão mais fraca da exogeneidade (\ref{eqn:cov}). Por outro lado, a segunda premissa diz que a multiplicação matricial de $\textbf{X}$ e sua transposta tem que ser igual a ordem de $\textbf{X}$ (\ref{eqn:posto}), implicando independência linear.

```{=tex}
\label{eqn:cov}
\begin{equation}
E[\textbf{X}'\mbfvarepsilon] = \textbf{0}
\end{equation}
```

```{=tex}
\label{eqn:posto}
\begin{equation}
E[\textbf{X'X}] = k
\end{equation}
```

*Ajuste do modelo*

Se chamarmos de $\hat{\mbfbeta}$ o vetor de parâmetros estimados por MQO, podemos obter o vetor $\hat{\textbf{Y}}$ de valores estimados de $\textbf{Y}$. Todos esses valores estimados estarão necessariamente sobre a reta de regressão do MQO. A diferença entre $\hat{\textbf{Y}}$ e $\textbf{Y}$ será justamente o vetor de resíduos $\hat{\mbfvarepsilon}$. Ou seja, $\textbf{Y} - \hat{\textbf{Y}} = \hat{\mbfvarepsilon}$. Logo, se pensarmos em cada observação $i$, $\hat{\varepsilon_i}$ é a diferença entre $y_i$ e $\hat{y_i}$. Se $\hat{\varepsilon_i} > 0$, a reta subestima $y_i$; se $\hat{\varepsilon_i} < 0$ a reta superestima $y_i$; se $\hat{\varepsilon_i} = 0$, a reta passa exatamente sobre $y_i$.

Pela propriedade da equação (\ref{eqn:expectations}), temos que a média dos resíduos é zero. De forma equivalente, a média dos valores estimados é a mesma média amostral dos valores observados, $\overline{\hat{y}} = \overline{y}$. Dada essa característica, podemos definir medidas de perturbação em relação à média amostral $\hat{y}$:

```{=tex}
\label{eqn:sqt}
\begin{equation}
SQT = \sum^{n}_{i = 1} (y_i - \overline{y})^2
\end{equation}
```

```{=tex}
\label{eqn:sqe}
\begin{equation}
SQE = \sum^{n}_{i = 1} (\hat{y_i} - \overline{y})^2
\end{equation}
```

```{=tex}
\label{eqn:sqr}
\begin{equation}
SQR = \sum^{n}_{i = 1} \hat{\varepsilon_{i}}^2
\end{equation}
```

$SQT$ (Soma dos quadrados totais) mede a dispersão das $i$ observações ($y_i$), tendo a média amostral como centro. $SQE$ (Soma dos quadrados estimados) mede a dispersão das $i$ estimativas de $y$ ($\hat{y_i}$). $SQR$ (Soma dos quadrados dos resídiuos) mede a variação dos erros estimados ($\hat{\varepsilon_i}$). A variação total em $y$ pode ser sempre espressada como a soma da variação explicada e da variação não explicada (ou dos resíduos):

```{=tex}
\label{eqn:soma}
\begin{equation}
SQT = SQE + SQR
\end{equation}
```

*Grau de Ajuste*

As definições acima nos ajudarão à definir uma medida de ajuste do modelo, ou seja, dizer quão bem nossas variáveis independentes ($\textbf{X}$) explicam as observações da variável dependente ($\textbf{Y}$).

Se assumirmos que $SQT \neq 0$, podemos definir uma medida conhecida como $R^2$ (coeficiente de determinação), que basicamente é uma proporção entre a variação explicada (SQE) e a variação total (SQT). Em outras palavras, é uma medida que diz quanto da variação total em $y$ foi explicada pelas variáveis $x$.

```{=tex}
\label{eqn:rquadrado}
\begin{equation}
R^2 = \frac{SQE}{SQT} = 1 - \frac{SQR}{SQT}
\end{equation}
```

$$R^2 \in [0, 1]$$

Como $SQE \leq SQT$, $R^2$ sempre estará entre $0$ e $1$. Graficamente falando, quanto mais próximo de $1$, mais próximas as observações estarão da reta de regressão.

# FORMAS FUNCIONAIS, INFERÊNCIA E QUEBRA DE PRESSUPOSTOS

*Multicolinearidade*

A multicolinearidade ou colinearidade é definida como uma correlação entre variáveis dispostas em um modelo. Em síntese, podemos separá-la em dois casos: multicolinearidade exata e multicolinearidade prejudicial.

A multicolinearidade exata, também conhecida como multicolinearidade perfeita, é quando as variáveis são exatamente correlacionadas entre si, ou seja, a correlação é de 100%. Nesse caso específico, é ferido um dos pressupostos do MCRL e esta falha é tão grave que, nesse caso, não é possível estimar os parâmetros dos modelos, uma vez que a matriz de variáveis independentes é singular. Destaca-se, no entanto, que esta correlação exata só é problemática quando é linear, ou seja, uma variável pode ter uma correlação exata quadrática, e.g., que não causará danos aos pressupostos do MCRL.

Felizmente, casos de multicolinearidade perfeita, na prática, são raros, mas a multicolinearidade prejudicial, com frequência, acontece. Nesse caso, por sua vez, a correlação não é exata, mas alta o suficiente para gerar problemas. Não há falha nos pressupostos do modelo, mas pode ser que surjam problemas estatísticos potencialmente graves, sobretudo no que diz respeito à eficiência do modelo. Algumas problemáticas que podemos citar são: grandes oscilações nas estimativas dos parâmetros; os erros padrão dos coeficientes podem ser muito altos, fazendo com que as estatísticas $t$, por exemplo, não sejam confiáveis, dado os baixos níveis de significância; pode ser que haja alteração no sinal do parâmetro ou magnitudes irreais.

Como a variância de um estimador $\beta_{k}$ qualquer é dada por:

$$
Var[\beta_{k} | X] = \frac{\sigma^2}{(1 - R_{k}^{2})\sum_{i = 1}^{n} (x_{ik} - \overline{x}_{k})^{2}}
$$

Onde $R^{2}_{k}$ é o $R^{2}$ na regressão de $X_{k}$ em relação a todas as outras variáveis. Assim, de $X_{k}$ é altamente correlacionada com as variáveis, $R^{2}_{k}$ tende a 1, de modo que a variância de bk tende a infinito. Caso $R^{2}_{k}$ seja igual a zero, então a condição de ortogonalidade é satisfeita e não há multicolinearidade no modelo. Assim, uma forma de detectar a multicolinearidade é analisar o fator $1/(1-R^{2}_{k})$, conhecido como fator de inflacionamento da variância (VIF), pois, se ele for muito alto, geralmente, valores superiores a 20, há multicolinearidade prejudicial, que trará os problemas estatísticos mencionados acima.

Alguns métodos para tentar solucionar este problema são: i) obter mais dados, ou seja, aumentar o tamanho da amostra, haja vista que isso aumentará a variabilidade de $X$; ii) transformar as variáveis, como, por exemplo, utilizar uma variável instrumental; iii) retirar do modelo as variáveis suspeitas de causar o problema. Este terceiro método é o mais óbvio e, com frequência, o mais utilizado, todavia, pode ser problemático, uma vez que, se essas variáveis retiradas pertencem ao modelo populacional, suas informações irão para o termo de perturbação, causando um viés que será explicado posteriormente, conhecido como viés de variável omitida.

*Inferência*

São três as principais funções do modelo de regressão linear. Em síntese, ele pode ser usado para estimação, testes de hipótese e previsão. O objetivo desta parte do trabalho, no entanto, é focar em algumas aplicações dos testes de hipóteses, ressaltando a importância da inferência estatística. Uma abordagem comumente vista para testar hipóteses são modelos cujos parâmetros possuem restrições. Em modelos restritos, por imposição teórica, por exemplo, temos que dentro do modelo irrestrito, apenas alguns serão válidos, isto é, apenas os modelos cuja restrição seja satisfeita. Nesse sentido, dizemos que os modelos são aninhados, ou seja, os restritos estão contidos dentro do irrestrito.

Os testes de hipóteses podem ser abordados a partir de dois pontos de vista. Em primeiro lugar, tendo calculado um conjunto de estimativas de parâmetros, é possível verificar se o fracasso das estimativas em satisfazer as restrições é apenas um erro de amostragem ou é algo sistemático. Por outro lado, uma outra abordagem seria considerar o modelo de mínimos quadrados irrestritos, com restrições tacitamente teóricas, de modo que, tal fato poderia levar a uma perda de ajuste do modelo, visto que ele não é adequado. Desta forma, é possível verificar se essa perda de ajuste é, novamente, um erro de amostragem ou se é tão relevante que levantaria dúvidas sobre a validade das restrições e, por consequência, do modelo em si. Apesar de serem consideradas separadamente, essas suas abordagens são equivalentes e servem para o mesmo propósito. Um importante e central ponto é que, para realizar as inferências, serão assumidos que os erros são normalmente distribuídos.

*O teste $F$*

Em primeiro lugar, é necessário definir uma hipótese nula e uma hipótese alternativa, a fim de poder fazer conclusões sobre o teste. Essas hipóteses são comumente tratadas como $H_{0} e $H_{1}$, respectivamente. O objetivo do teste, portanto, é aceitar ou não (esse caso, na maioria das vezes, implica aceitar a hipótese alternativa) a hipótese nula. Um método bem conhecido de testar $H_{0}$ é o chamado Critério de Wald:

$$
W = \textbf{m}'{Var[\textbf{m} | \textbf{X}]}^{-1}\textbf{m}
$$

$$
= (\textbf{Rb} - \textbf{q})'[\sigma^2 \textbf{R}(\textbf{X'X})^{-1}\textbf{R'}]^{-1}(\textbf{Rb} - \textbf{q})
$$

$$
= \frac{(\textbf{Rb} - \textbf{q})'[\textbf{R}(\textbf{X'X})^{-1}\textbf{R'}]^{-1}(\textbf{Rb} - \textbf{q})}{\sigma^2} \sim \chi^2[J]
$$

Essa estatística consiste em uma distribuição $\chi^2$ com $J$ graus de liberdade se a hipótese estiver correta. Sua interpretação intuitiva é de que quanto maior for o fracasso dos MQ em satisfazer as restrições, maior será o valor da estatística, i.e, maior o valor $\chi^2$. Desse modo, um alto valor diminuirá a probabilidade de se aceitar $H_0$. Entretanto, empiricamente este teste não é aplicável, uma vez que ele trabalha com a variância populacional, desconhecida. Todavia, existe uma forma de contornar este problema, transformando a estatística $W$ em um teste $F$. Para isso, é necessário dividir $W$ por seus graus de liberdade e utilizar a variância estimada, que é conhecida, em vez da variância populacional:

$$
F = \frac{W}{J} \frac{\sigma^2}{s^2}
$$

$$
= \Biggl (\frac{(\textbf{Rb} - \textbf{q})'[\textbf{R}(\textbf{X'X})^{-1}\textbf{R'}]^{-1}(\textbf{Rb} - \textbf{q})}{\sigma^2}\Biggl) \Biggl ( \frac{1}{J}\Biggl ) \Biggl ( \frac{\sigma^2}{s^2}\Biggl ) \Biggl ( \frac{n-K}{n-K}\Biggl )
$$

$$
= \frac{(\textbf{Rb} - \textbf{q})'[\sigma^2\textbf{R}(\textbf{X'X})^{-1}\textbf{R'}]^{-1}(\textbf{Rb} - \textbf{q})/J}{[(n - K)s^2/\sigma^2]/(n - K)}
$$


Após as devidas transformações, chegamos ao teste $F$, de fato, que é uma estatística na qual ocorre a divisão de duas qui-quadrado, independentes, e ambas divididas pelos seus respectivos graus de liberdade.

$$
F[J, n - K] = \frac{(\textbf{Rb} - \textbf{q})'\{\textbf{R}[s^2(\textbf{X'X})^{-1}]\textbf{R'}\}^{-1}(\textbf{Rb} - \textbf{q})}{J}
$$

Ademais, deve-se salientar que, para testar uma única restrição, a estatística $F$ se equivale ao quadrado da estatística $t$, dada por:


$$
t^2 = \frac{(\hat{q} - q)^2}{Var(\hat{q} - q | \textbf{X})}
$$

$$
= \frac{(\textbf{r'b} - q)\{\textbf{r'}[s^2(\textbf{X'X})^{-1}]\textbf{r}\}^{-1}(\textbf{r'b} - \textbf{q})}{1}
$$

*O estimador de mínimos quadrados restritos*

O estimador de mínimos quadrados restritos, neste caso, é igual ao estimador de mínimos quadrados irrestritos, mais um termo que explica a ineficiência da solução irrestrita em satisfazer as condições de restrição. Em outras palavras, o estimador irrestrito, caso haja imposição de restrições, é viesado e seu viés é muito parecido com viés causado por uma variável relevante omitida, como será mostrado mais a frente.

Nesse sentido, o teste F permite fazer a comparação entre os modelos, de modo a verificar qual é o mais adequado entre o restrito e o irrestrito. Essa comparação, de modo geral, consiste em analisar o $R^2$ dos modelos, ponderados pelos seus graus de liberdade, da forma que se segue:

$$
F[J, n - K] = \frac{(R^2 - R^2_{*})/J}{(1 - R^2)/(n - K))}
$$

*Formas funcionais e variáveis dummies*

Um modelo de regressão linear pode ser adaptado para situações particulares e específicas da natureza de um fenômeno. Essa adaptação pode ser feita em sua forma funcional, que define a relação entre a variável observada $Y$ e suas explicativas $X_i$ em um modelo de regressão. A forma funcional adequada é a que captura com precisão a natureza da relação entre as variáveis.

A forma funcional mais básica é a linear, em que a variável explicada $Y$ depende das demais variáveis $X$ sem nenhuma transformação, como apresentado nas seções anteriores:

$$\textbf{Y} = \textbf{X} \mbfbeta + \mbfvarepsilon$$

Além do caso linear, temos uma forma funcional que utiliza variável binária ou dummy. Uma variável dummy assume o valor 1 para algumas observações para indicar a presença de um efeito ou participação em um grupo e 0 para as observações restantes. As variáveis binárias são um meio conveniente de construir mudanças discretas da função em um modelo de regressão. Variáveis dummy são geralmente usadas em equações de regressão que também contêm outras variáveis quantitativas:

$$\textbf{Y} = \textbf{X} \mbfbeta + \textbf{D} \mbfgamma + \mbfvarepsilon$$

Expandindo a equação anterior:

```{=tex}
\begin{equation}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}_{n \times 1}
=
\begin{bmatrix}
1 && x_{12} && \cdots && x_{1k} \\
1 && x_{22} && \cdots && x_{2k} \\
\vdots && \vdots && \ddots && \vdots \\
1 && x_{n2} && \cdots && x_{nk}
\end{bmatrix}_{n \times k}
\cdot
\begin{bmatrix}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k \\
\end{bmatrix}_{k \times 1}
+
\begin{bmatrix}
0 && \cdots && 1 \\
1 && \cdots && 0 \\
\vdots && \ddots && \vdots \\
1 && \cdots && 1
\end{bmatrix}_{n \times \theta}
\cdot
\begin{bmatrix}
\gamma_1 \\
\gamma_2 \\
\vdots \\
\gamma_{\theta} \\
\end{bmatrix}_{\theta \times 1}
+
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_{n}
\end{bmatrix}_{n \times 1}
\end{equation}
```
Sendo $\textbf{D}$ uma matriz formada por zeros (0) e uns (1) que indicam que uma dada observação possui alguma característica ou pertence a um grupo específico. Essa matriz tem dimensão $n \times \theta$, em que $n$ é o tamanho da amostra e $\theta$ o número de dummies. O vetor coluna $\mbfgamma$ são os coeficientes de regressão associados as $\theta$ dummies.

Quando uma dummy recebe valor 1, então o valor de $\gamma$ associado é incorporado na estimação. Caso contrário, a dummy será 0 e não incluirá o valor de $\gamma$.

No caso de variáveis categóricas com múltiplas categorias, podemos representá-las por dummies e omitir uma das categorias na forma funcional, para que não tenhamos multicolinearidade perfeita:

$$y = \beta_0 + \beta_1 x_1 + \gamma_1 primavera + \gamma_2 outuno + \gamma_3 verao$$

No exemplo acima, a variável categórica tenta captar o efeito da estação do ano na variável observada $y$. Note que não criamos uma dummy para inverno, pois essa foi omitida para ser a base de comparação, no caso em que todas assumam valor 0.

Outros exemplos de formas funcionais são as que aplicam transformações não lineares em variáveis numéricas. As mais comuns são:

Regressão quadrática, em que a uma ou mais variáveis dependentes tem uma relação quadrática com a variável explicada:

$$y = X \beta + X^{2} \gamma + \varepsilon$$

Regressão Log-linear, em que a varíavel explicada e as variáveis dependentes crescem de forma exponencial e se deseja suavizar ou linearizar a relação:

$$ln(y) = ln(X) \beta + \varepsilon$$

Regressão Semi-logarítima, em que apenas a variável explicada é exponencial e se deseja linearizar:

$$ln(y) = X \beta + \varepsilon$$

Regressão de efeitos interativos, utilizada quando se deseja mensurar a interação entre duas variáveis, adicionando-se um coefieciente entre o produto dessas variáveis:

$$y = X \beta + \beta_{\tau} (x_{\alpha} \cdot x_{\delta}) + \varepsilon$$

$\beta_{\tau}$ mede a interação entre as variáveis dependentes na estimação de $y$.

*Análise da qualidade de ajuste e escolha de modelos*

Quando há mais de um candidato para a escolha de um modelo, deve-se seguir alguns testes com objetivo de verificar qual o mais adequado. Nesse sentido, nesta parte serão discutidos os impactos de uma má especificação, bem como os métodos estatísticos para realizar a escolha quando têm-se dois ou mais modelos.

Um dos erros mais comuns de se ver em especificação de modelos são a omissão de variáveis relevantes e a inclusão de variáveis irrelevantes. Suas implicações, contudo, são bem diversas. Quando omitimos uma variável relevante de um modelo, automaticamente geramos um viés no estimador, haja vista que os efeitos dessa variável vão para o termo de perturbação. Um problema ainda mais inquietante é quando o pesquisador se depara com um modelo com mais de uma variável explicativa e duas ou mais dessas variáveis possuem uma alta correlação entre si. Ocorrendo isso, há o dilema entre retirar uma variável que pode ser relevante ou deixa-lá e viesar o estimador, haja vista a alta colinearidade. Quando tal problema ocorre, é comum a criação de um outro estimador, conhecido como estimador pré teste. Todavia, este estimador também se revela viesado e a experiência mostra que ele costuma ser o menos preciso entre o estimador com variável (eis) omitida (s) e o que considera as variáveis que gera colinearidade prejudicial.

Totalmente distinta a omissão de variáveis relevantes está a inclusão de variáveis irrelevantes, haja vista que essa não gera um viés de estimador, uma vez que o que ocorre é que, se a variável é supérflua, seu parâmetro estimado será igual a zero. Um problema empírico só ocorre do ponto de vista da variável irrelevante ao modelo ser altamente correlacionada com variáveis relevantes, pois, como dito anteriormente, isso poderia causar imprecisão das estimativas.

*Construindo um modelo - Estratégias gerais*

Com os avanços de hardwares e softwares, houve uma mudança na construção dos modelos durante os últimos vinte anos. Dado o estágio atual da tecnologia, os pesquisadores se mostram mais confortáveis em iniciar suas pesquisas com modelos grandes e elaborados, o que é justificável, visto que, como vimos, omitir uma variável relevante é bem mais problemático que acrescentar uma variável irrelevante. Todavia, ainda são necessários testes para verificar qual o modelo mais correto para determinada abordagem.

Tratando-se de modelos não aninhados, ou seja, modelos que não são mutuamente excludentes, os testes clássicos tornam-se mais complicados de serem aplicados. A partir disso, uma atenção geral para essa problemática foi dada por teóricos e empiristas. Uma abordagem bastante chamativa, nesse sentido, é a abordagem bayesiana, uma vez que permite a comparação de duas hipóteses (dois modelos), em vez da validade de uma sobre a outra. Em outras palavras, não há verdade absoluta, o método gera uma razão de probabilidade de um modelo ser melhor, em detrimento do outro.

Entretanto, ao focar nos testes clássicos deve-se salientar que estes, em geral, buscam evidências para refutar a hipótese nula, isto é, rejeitar $H_0$. Mas, como considerar qual é $H_0$ e qual é $H_1$? Para isso, existe a metodologia Neyman-Pearson, a qual sugere que a hipótese nula seja o modelo mais estreito do conjunto considerado. Há um grande problema, no entanto: os modelos clássicos nunca chegam a uma conclusão certeira. Isso porque sempre existem possibilidades do erro tipo 1. Em outras palavras, existe um intervalo de confiança que permite rejeitar $H_0$, mas este intervalo é sempre menor que um.

Nesse sentido, o primeiro trabalho sobre a escolha de modelos não aninhados foi o de Cox (1961, 1962), baseado em princípios de máxima verossimilhança. Uma abordagem mais recente é baseada no chamado princípio da abrangência. Basicamente, esse princípio procura determinar se o modelo escolhido é capaz de abranger as características de seus concorrentes, i.e, se o modelo é capaz de explicar a hipótese alternativa.

Mais especificamente, um modelo abrangente é um modelo que explica características especiais do seu concorrente. Assim, para realizar o teste, uma possibilidade é o aninhamento artificial dos dois modelos, isto é, juntar suas variáveis próprias e as variáveis que possuem em comum em um só modelo.

$$
Y = \overline{\textbf{X}} {\overline{\mbfbeta}} + \overline{\textbf{Z}} \overline{\mbfgamma} + \textbf{W} \mbfdelta + \mbfvarepsilon
$$

Todavia, essa possibilidade pode acarretar em uma série de problemas, como alto número de regressores, o que seria problemático para um cenário de série temporal, de modo que poderia causar problemas de colinearidade. Além disso, como um regressor é composto por partes dos regressores dos dois modelos, um teste $F$ seria impreciso, logo não seria possível rejeitar $H_0$ ou $H_1$ de fato. Uma outra forma mais interessante é supor que, digamos $H_0$ seja a hipótese correta, pois, nesse sentido, a variável explicada dependeria apenas de um dos regressores mais o termo de erro. Para tanto, podemos gerar um modelo contando com as variáveis singulares de cada um e testar se o regressor do modelo que corresponde a $H_1$ é igual a zero, o que indicaria aceitar a hipótese nula. Em outros termos, seria como se estivéssemos adicionando uma variável irrelevante ao modelo, haja vista que o modelo de $H_0$ já daria conta de explicar.

Uma proposição alternativa é o teste $J$, proposto por Davidson and MacKinnon (1981), que traz as implicações deste modelo para a regressão linear:

$$
\textbf{y} = (1 - \lambda)\textbf{X}\mbfbeta + \lambda(\textbf{Z}\mbfgamma) + \mbfvarepsilon
$$

Assim, se $\lambda$ é igual a zero, devemos rejeitar $H_1$. O problema é que não é possível estimar lambda separadamente, assim o teste $J$ consiste em estimar um parâmetro por uma regressão de MQ de $y$ em $Z$ seguida por uma regressão de $y$ em $X$. Outra possibilidade é testar $\lambda$ assintoticamente, mas isso não garante um resultado exatamente robusto para amostras finitas.

Outros testes comumente utilizados são os testes de razão de verossimilhança. Os testes de razão de verossimilhança são fundamentados em três características importantes da densidade da variável aleatória de interesse. Primeiramente, sob a hipótese nula, a densidade logarítmica média é esperada ser menor do que na alternativa, o que decorre do fato de que o modelo nulo está contido dentro da alternativa. Em segundo lugar, os graus de liberdade para a estatística qui-quadrado são determinados pela redução na dimensão do espaço de parâmetros especificado pela hipótese nula, comparado com a alternativa. Terceiro, para que o teste seja válido, sob a hipótese nula, a estatística de teste deve seguir uma distribuição conhecida que seja independente dos parâmetros do modelo sob a hipótese alternativa. Quando os modelos não são aninhados, nenhum desses requisitos é completamente satisfeito. Primeiramente, a diferença na densidade logarítmica média pode não ser mantida. Em segundo lugar, o espaço de parâmetros no modelo nulo pode ser maior ou do mesmo tamanho que na alternativa, o que complica a interpretação dos graus de liberdade. Terceiro, as distribuições das estatísticas de teste baseadas em probabilidade geralmente se tornam funções dos parâmetros do modelo alternativo, devido à simetria das hipóteses nula e alternativa.

Para resolver essas questões, a análise de Cox produziu uma reformulação estatística de teste baseada na distribuição normal padrão e centrada em zero. Essa abordagem foi posteriormente adaptada por Pesaran e Deaton para modelos de regressão linear e não linear. Posteriormente, Evans e Deaton estenderam essa estatística de teste para modelos lineares versus modelos log-lineares. É interessante notar que, como nos modelos de regressão clássicos, o estimador de mínimos quadrados é também o estimador de máxima verossimilhança, o que sugere uma relação entre esses diferentes métodos de teste. Por exemplo, Davidson e MacKinnon descobriram que sua estatística de teste é assintoticamente igual ao negativo da estatística Cox-Pesaran e Deaton.

$$
c_{01} = \frac{n}{2}ln \Biggl [ \frac{s^2_{\textbf{Z}}}{s^2_{\textbf{X}} + (1/n)\textbf{b'X'}\textbf{M}_{\textbf{Z}} Xb} \Biggl ] = \frac{n}{2} ln \Biggl [\frac{s^2_{Z}}{s^2_{ZX}} \Biggl ]
$$

sendo:

$$
\textbf{M}_{\textbf{Z}} = 1 - \textbf{Z(Z'Z)}^{-1}\textbf{Z'},
$$

$$
\textbf{M}_{\textbf{X}} = 1 - \textbf{X(X'X)}^{-1}\textbf{X'},
$$

$$
\textbf{b} = \textbf{(X'X)}^{-1}\textbf{X'y},
$$

$$
s^2_{\textbf{Z}} = \textbf{e'}_{\textbf{Z}}\textbf{e}_{Z}/n,
$$

$$
s^2_{\textbf{X}} = \textbf{e'}_{\textbf{X}}\textbf{e}_{X}/n,
$$

$$
s^2_{\textbf{ZX}} = s^2_{\textbf{X}} + \textbf{b'}\textbf{X'}\textbf{M}_{\textbf{Z}}\textbf{Xb},
$$

A hipótese é testada ao comparar o valor crítico com a tabela normal. Um alto valor de $q$ é uma evidência para rejeitar a hipótese nula.

$$
q = \frac{c_{01}}{\sqrt{\frac{s^2_{\textbf{X}}}{s^2_{\textbf{ZX}}}\textbf{b'}\textbf{X'}\textbf{M}_{\textbf{Z}}\textbf{M}_{\textbf{X}}\textbf{M}_{\textbf{Z}}\textbf{Xb}}}
$$

*Heterocedasticidade*

A heterocedasticidade, um fenômeno estatístico comum em diversos tipos de dados, se caracteriza pela desigualdade na variância dos erros entre as diferentes observações. Essa variação pode ocorrer tanto em dados transversais quanto em séries temporais.

Em séries temporais de alta frequência, como as cotações diárias no mercado financeiro, a volatilidade dos dados é um indicativo da heterocedasticidade. Já nos dados transversais, a heterocedasticidade se manifesta quando a escala da variável dependente e o poder explicativo do modelo variam entre as observações. Um exemplo clássico são os dados microeconômicos, como pesquisas sobre gastos de consumo. Mesmo considerando o tamanho das empresas, espera-se observar uma maior variabilidade nos lucros de grandes empresas em comparação com as menores. Essa variabilidade também pode ser influenciada por fatores como diversificação de produtos, investimentos em P&D, características do setor de atuação, entre outros, gerando diferenças inclusive entre empresas de porte similar.

Ao analisarmos os padrões de despesa familiar, notamos que a variabilidade das despesas em determinados grupos de bens é maior entre famílias de alta renda do que entre as de baixa renda. Essa diferença se deve à maior discricionariedade proporcionada por uma renda mais elevada.

É importante salientar que, mesmo com a presença de heterocedasticidade, as perturbações (ou erros) ainda são consideradas não correlacionadas entre si.

Se a matriz de covariância é escrita na forma $E[\mbfvarepsilon \mbfvarepsilon' | \textbf{X}] = \sigma^2\Omega$, podemos dizer que no caso homocedástico $\Omega = I$. Se $\Omega \neq I$, então os resíduos são heterecedásticos ou autocorrelacionados ou ambos.

Portanto, o estimador de mínimos quadrados é:

$$
\textbf{b} = \mbfbeta + (\textbf{X'X})^{-1}\sum_{i=1}^{n} \textbf{x}_{i}\varepsilon_{i}
$$

e sua matriz de covariância é:

$$
Var[\textbf{b} | \textbf{X}] = \frac{1}{n} \Biggl ( \frac{\textbf{X'X}}{n} \Biggl)^{-1} \Biggl ( \frac{\textbf{X'}(\sigma^2\Omega)X}{n} \Biggl) \Biggl ( \frac{\textbf{X'X}}{n} \Biggl)^{-1}
$$

Logo, vemos que $s^2(\textbf{X'X})^{-1}$ não é um estimador apropriado para a matriz de covariância assintótica para o estimador de MQ. Para resolver essa questão, White (1980, 2001) mostrou que, sob condições gerais, o estimador:

$$
S_0 = \frac{1}{n} \sum_{i=1}^{n}e^2_{i}\textbf{x}_{i}\textbf{x}_{i}'
$$

converge assintóticamente para a matriz $Q_{*}$ de soma de quadrados e produtos vetoriais que envolvem $\sigma_{ij}$ e as linhas de $X$:

$$
Q = \Biggl ( \frac{\textbf{X'}(\sigma^2\Omega)X}{n} \Biggl)
$$

Dessa forma, obtemos o estimador consistente de White:

$$
Asy. Var[\textbf{b} | \textbf{X}] = n(\textbf{X'X})^{-1}S_0(\textbf{X'X})^{-1}
$$

que ser usado para estimar a matriz de covariância assintótica de b. Este resultado implica que, sem realmente especificar o tipo de heterocedasticidade, ainda podemos fazer inferências apropriadas com base no estimador de mínimos quadrados. Esta implicação é útil se não tivermos certeza da natureza da heterocedasticidade.

# ENDOGEINEIDADE, VARIÁVEIS INSTRUMENTAIS E SISTEMAS DE EQUAÇÕES

*Causas da endogeneidade e métodos de contorno*

A classificação dentro de um modelo entre variáveis exógenas e endógenas é muitas vezes controversa. A endogeneidade ocorre quando as variáveis independentes estão correlacionadas com os termos de erro do modelo de regressão. Isso gera problemas que podem ser graves, uma vez que, um “choque” influencia tanto a variável explicada quanto a variável explicativa. Desse modo, em um modelo que sofre deste problema, os estimadores não seriam apenas viesados como, também, inconsistentes.

Nesse sentido, existem vários motivos que podem causar a endogeneidade como: variáveis omitidas, equações simultâneas, má especificação de modelos e etc. Quando trata-se de variável omitida, desde que seja relevante, seus efeitos irão estar contidos no termo de perturbação e, caso essa variável seja correlacionada com a variável explicativa, haverá endogeneidade e, logo, viés e inconsistência das estimativas. Outro caso clássico é o caso de equações simultâneas, onde há um sistema de equações que relacionam variáveis. Na economia esse problema é muito comum, haja vista a necessidade de equilíbrio, que envolve mais de uma equação. O multiplicador Keynesiano é um exemplo, uma vez que a variável de consumo é endógena. Outro exemplo frequente é o da igualdade entre equações de oferta e demanda, nas quais o preço sempre é endógeno. Por fim, também pode-se haver problemas empíricos com a escolha de uma variável explicativa, a qual pode não ser eficaz e, desse modo, estar correlacionada com uma variável que de fato seria e, portanto, está introduzida no distúrbio da equação.

Dessa forma, nota-se que há várias razões que podem causar problemas de endogeneidade em um modelo e são necessários métodos que corrijam esses problemas, haja vista as imprecisões que são causadas nas estimativas. Obviamente, esses métodos já existem e são capazes de contornar este transtorno. Estimação por variáveis instrumentais, mínimos quadrados em dois estágios e pelo método de momentos podem solucionar a endogeneidade e, com isso, fornecer estimativas robustas. Neste trabalho, no entanto, focaremos nos métodos de estimação por variáveis instrumentais e por mínimos quadrados em dois estágios (MQ2E). 

*Variáveis instrumentais*

As variáveis instrumentais (VI) são utilizadas para resolver problemas de endogeneidade que surgem quando uma ou mais variáveis explicativas estão correlacionadas com o termo de erro no modelo de regressão. Isso viola a premissa básica da regressão linear clássica, onde as variáveis explicativas devem ser exógenas, ou seja, não correlacionadas com o termo de erro. O objetivo aqui é discutir como os métodos de variáveis instrumentais (VI) são utilizados para contornar os problemas de endogeneidade, permitindo a obtenção de estimadores consistentes para os parâmetros de interesse em um modelo de regressão, bem como isso é feito.

As variáveis instrumentais podem ser utilizadas para: i) lidar com endogeneidade quando há correlação entre as variáveis explicativas e o erro. Isso ocorre frequentemente devido ao viés de variável omitida ou erros nas variáveis, o que torna os estimadores obtidos por métodos tradicionais inconsistentes. As VI ajudam a obter estimativas consistentes ao fornecer uma fonte exógena de variação em que não está correlacionada com o erro, resolvendo esses problemas. Em termos algébricos, sempre que  $Cov(x,u) \neq 0$; ii) resolver os problemas de variáveis omitidas ao permitir isolar a variação exógena de que não é contaminada pelo termo de erro, eliminando o viés que surge quando uma variável relevante é omitida da regressão; iii) corrigir a inconsistência introduzida pelos erros de medição nas variáveis explicativas, substituindo a variável endógena medida com erro por uma previsão baseada nos instrumentos exógenos; iv) comparação com variáveis proxy, pois embora essas variáveis possam ser usadas quando não há boas VI disponíveis, elas podem estar correlacionadas com o termo de erro e, portanto, ainda levar a estimativas viesadas. As VI são preferíveis porque, quando adequadamente selecionadas, satisfazem as condições de relevância e exogeneidade, resultando em estimadores mais robustos.

Seja o modelo de regressão simples em sua forma mais geral:

$$y = \beta_0 + \beta_1 x_1 + u$$
I) O instrumento $z$ deve ser exógeno. Isto implica que $Cov(z,u) = 0$

II) O instrumento $z$ deve estar correlacionado com a variável endógena $x_1$, ou seja, $Cov(z, x) \neq 0$.

De modo mais claro, a variável instrumental deve ser: i) exógena, ou seja, não deve estar correlacionada com o termo de erro do modelo de regressão. Essa condição garante que a VI não carrega a influência de variáveis não observadas que afetam a variável dependente e que podem introduzir viés nas estimativas dos parâmetros: ii) relevante, isto é, a VI deve estar correlacionada com a variável endógena, o que significa que ela deve influenciar a variável explicativa de interesse. A relevância garante que a VI é um bom indicador da variável endógena, permitindo que sua variação seja usada para isolar a variação exógena.

Dessa forma, essas duas condições são essenciais para que a VI seja usada para obter estimativas consistentes dos coeficientes dos parâmetros do modelo de regressão, mesmo na presença de endogeneidade.

As Variáveis Instrumentais podem ser utilizadas tanto em modelos de regressão simples quanto em modelos de regressão múltipla. Em um modelo de regressão simples, o objetivo é estimar a relação entre uma variável dependente $y$ e uma variável explicativa $x$. Por outro lado, enquanto a abordagem de VI em um modelo de regressão simples envolve um único instrumento para uma única variável endógena, a regressão múltipla frequentemente lida com múltiplas variáveis explicativas, algumas das quais podem ser endógenas. Portanto, é necessário um conjunto mais robusto de instrumentos.

Embora a VI seja consistente quando $z$ e $u$ são não correlacionados, e $z$ e $x$ tem qualquer correlação, positiva ou negativa, as estimativas de VI podem ter grandes erros-padrão, especialmente se $z$ e $x$ forem apenas fracamente correlacionados. A fraca correlação entre $z$ e $x$ pode ter consequências ainda mais sérias, o estimador VI pode ter grande viés assintótico mesmo se $z$ e $u$ forem só moderadamente correlacionados. Ou seja, mesmo que tenhamos um instrumento válido teoricamente, empiricamente ele pode se revelar um instrumento ruim que pode gerar um viés assintótico ainda maior que o de MQO sob a condição de endogeneidade. Nesse caso, deve-se comparar qual método é mais efetivo e convenciona-se que se escolhe o método de VI caso a razão da correlação entre o instrumento e o termo de erro dividida pela correlação do instrumento com a variável endógena seja menor que a correlação da variável endógena com o erro.

*O Método dos Mínimos Quadrados em Dois Estágios (MQ2E)*

Como visto anteriormente, o método de VI utiliza de um único instrumento para controlar a endogeneidade em um modelo. No entanto, pode ser que hajam múltiplos instrumentos e, neste caso, recorremos ao MQ2E. Este método, em síntese, consiste em instrumentalizar as variáveis endógenas no chamado primeiro estágio e, posteriormente, regredir o modelo por MQO no segundo estágio.

A questão que distingue o método a qual se deve recorrer é a identificação da equação. Se ela é, por exemplo, superidentificada, isto é, quando há mais instrumentos do que variáveis endógenas, o MQ2E torna-se mais prático. Isto porque, estimando através de VI, seria possível obter diferentes estimadores possíveis, enquanto o MQ2E fornece exatamente um.

A identificação de uma equação é obtida pela condição de ordem e pela condição de posto. A condição de ordem diz que o número de variáveis ​​exógenas excluídas da equação deve ser pelo menos tão grande quanto o número de variáveis ​​endógenas incluídas na equação j. Essa condição é necessária, porém não suficiente. Já a condição do posto é necessária e suficiente e define que a matriz das variáveis explicativas escalonada deve ter o mesmo posto da matriz original, em outras palavras, essa matriz deve ser linearmente independente. Nesse sentido, uma equação é: i) superidentificada quando o número de instrumentos é maior que o número de variáveis endógenas; ii) exatamente identificada quando os instrumentos são da mesma magnitude das variáveis endógenas; iii) não identificada quando há mais variáveis endógenas que instrumentos.

Um problema importante que o MQ2E apresenta em relação ao MQO diz respeito a multicolinearidade prejudicial. Essencialmente, a variância do MQ2E tende a ser maior que a variância do MQO. Em primeiro lugar, porque a variável estimada tende a variar menos que a variável endógena e como a variação dessas variáveis estão inversamente relacionadas com a variância do estimador, isso significa uma variância maior. Além disso, como a variável estimada no primeiro estágio é uma combinação linear das variáveis exógenas, a correlação da estimativa com essas variáveis explicativas tende a ser, muitas vezes, maior, causando o problema da multicolinearidade prejudicial.

Outra questão que não pode ser negligenciada é a aplicação do teste F em modelos estimados por MQ2E. Isso ocorre porque o coeficiente de determinação R² pode ser negativo, de modo que a estatística F também o seja, prejudicando a análise dos resultados.

Desse modo, é notável que, apesar de ser consistente, o MQ2E possui suas limitações e, na ausência de endogeneidade, ele é menos eficiente que o MQO, além de poder trazer problemas em análises de inferências. Para tanto, testes de endogeneidade são fundamentais para verificar qual o estimador mais adequado.

Como dito anteriormente, a estimativa de Mínimos Quadrados em Dois Estágios é menos eficiente que Mínimos Quadrados Ordinários quando as variáveis explicativas são exógenas. Como observado, as estimativas MQ2E podem apresentar erros-padrão, que são a raiz da variância estimada, consideravelmente grandes. Portanto, um teste útil para verificar se há endogeneidade em um modelo é o teste de Hausman.

A intuição deste teste é bem simples: basicamente, deve-se encontrar a forma reduzida da variável em análise e, posteriormente, estimar os parâmetros por MQO, obtendo os resíduos dessa estimação. Logo após, deve-se estimar os parâmetros da equação estrutural de interesse incluindo os resíduos da estimação da equação reduzida e verificar se o parâmetro associado a este resíduo é significativo. Caso seja, a variável em análise é endógena e, portanto, o MQ2E é mais adequado.

*Modelo SUR*

O modelo SUR (*Seemingly Unrelated Model* ou Modelo aparentemente não relaciado) é um modelo composto por $M$ equações e $T$ observações e recebe esse nome pela possível correlação entre os resíduos das diferentes equações:

$$
\begin{bmatrix}
\textbf{y}_1 \\
\textbf{y}_2 \\
\vdots \\
\textbf{y}_M
\end{bmatrix}_{MT \times 1}
=
\begin{bmatrix}
\textbf{X}_1 && \textbf{0} && \textbf{0} && \textbf{0} \\
\textbf{0} && \textbf{X}_{2} && \textbf{0} && \textbf{0} \\
\vdots && \vdots && \ddots && \vdots \\
\textbf{0} && \textbf{0} && \textbf{0} && \textbf{X}_{M}
\end{bmatrix}_{MT \times MT}
\begin{bmatrix}
\mbfbeta_1 \\
\mbfbeta_2 \\
\vdots \\
\mbfbeta_M
\end{bmatrix}_{MT \times 1}
+
\begin{bmatrix}
\mbfvarepsilon_1 \\
\mbfvarepsilon_2 \\
\vdots \\
\mbfvarepsilon_M
\end{bmatrix}_{MT \times 1}
= \textbf{X} \mbfbeta + \mbfvarepsilon
$$

Note que na equação acima temos matrizes dentro de matrizes, ou seja, cada $\textbf{y}_i$ com $i \in \{1, 2, ..., M\}$ é uma matriz coluna da variável explicada com dimensão $T \times 1$, sendo $T$ o tamanho da amostra. Logo, $\mbfvarepsilon$ também é um vetor coluna com dimensão $MT \times 1$, composto por $M$ vetores de dimensão $T \times 1$.

Os modelos SUR são utilizados para analisar variáveis inter-relacionadas, como inflação, taxa de juros e crescimento econômico. A correlação entre os erros das equações pode capturar choques macroeconômicos comuns. Além disso, quando os dados envolvem múltiplas entidades (países, empresas) ao longo do tempo, os modelos SUR podem ser usados para modelar as equações para cada entidade de forma simultânea, capturando correlações nos erros que refletem influências comuns. São também, frequentemente usados para estudos de oferta e demanda.

Se assumirmos que $\textbf{X}_i$ é estritmente exógeno, temos que:

$$
E[\mbfvarepsilon | \textbf{X}_1, \textbf{X}_2, ..., \textbf{X}_M ] = \textbf{0}
$$

Se assumirmos homocedasticidade e resíduos não autocorrelacionados em cada equação, temos:

$$
E[\mbfvarepsilon_{m} \mbfvarepsilon_{m}' | \textbf{X}_1, \textbf{X}_2, ..., \textbf{X}_M ] = \mbfsigma_{mm} \textbf{I}_{T}
$$

A suposição de exogeneidade estrita utilizada acima é mais forte do que necessário. Pode-se assumir que $E[\mbfvarepsilon_m \mid \textbf{X}_m] = 0$, permitindo correlação entre distúrbios de diferentes equações. Um total de $T$ observações envolve $K_m$ regressores, com $K = \sum_{m=1}^{M} K_m$. Exigimos $T \geq \sum_{m=1}^{M} K_m$ observações para estimar os parâmetros das $M$ equações. Assumimos que os distúrbios não são correlacionados ao longo do tempo, mas podem ser correlacionados entre as equações no mesmo ponto temporal. Logo:

$$
E[\mbfvarepsilon \mbfvarepsilon' \mid \textbf{X}_1, \textbf{X}_2, ..., \textbf{X}_M ] = \Omega =
\begin{bmatrix}
\mbfsigma_{11}\textbf{I} && \mbfsigma_{12}\textbf{I} && \cdots && \mbfsigma_{1M}\textbf{I} \\
\mbfsigma_{21}\textbf{I} && \mbfsigma_{22}\textbf{I} && \cdots && \mbfsigma_{2M}\textbf{I} \\
\vdots && \vdots && \ddots && \vdots \\
\mbfsigma_{M1}\textbf{I} && \mbfsigma_{M2}\textbf{I} && \cdots && \mbfsigma_{MM}\textbf{I}
\end{bmatrix}
=
\Sigma \otimes \textbf{I}
$$

No caso em que não há autorrelação residual, temos um modelo idêntico ao MCLR que pode ser estimado por MQO:

$$
\textbf{b} = (\textbf{X}'\textbf{X})^{-1} \textbf{X}'y \implies \textbf{b}_m = (\textbf{X}_m' \textbf{X}_m)^{-1}\textbf{X}_m ' \textbf{y}_m.
$$

Portanto:

$$
\textbf{b}_m = \mbfbeta_m + (\textbf{X}_m' \textbf{X}_m)^{-1}\textbf{X}_m ' \mbfvarepsilon_m
$$

No caso em que temos $\Omega = \Sigma \otimes \textbf{I}$, relaxando o pressuposto de homocedasticidade, podemos estimar o modelo SUR por meio do Método dos Mínimos Quadrados Generalizados (MQG):

$$
\hat{\beta} = [\textbf{X}'\Omega^{-1}X]^{-1}X'\Omega^{-1}\textbf{y} = [\textbf{X}'(\Sigma^{-1} \otimes I)^{-1}X]^{-1}X'(\Sigma^{-1} \otimes I)^{-1}\textbf{y}
$$

# APLICAÇÃO

Para aplicar as questões tratadas nas últimas seções, vamos estimar um modelo linear por MQO, utilizando a linguagem R e os dados da REGIC 2018 para municípios de Minas Gerais.

Carregando pacotes:
```{r}
library(readxl)
library(dplyr)
library(ggplot2)
library(stargazer)
library(geobr)
library(ggspatial)
library(ggrepel)
library(scatterplot3d)
library(texreg)
library(systemfit)
library(sandwich)
library(ivreg)
library(lmtest)
```

Carregando a base da REGIC 2018 e ajustando as variáveis de interesse:

```{r}
df_regic <- readxl::read_xlsx(
  path = "data/REGIC2018 Cidades v2.xlsx",
  sheet="Base de dados por Cidades"
) |>
  dplyr::filter(
    UF == "MG"
  ) |>
  dplyr::select(
    "COD_CIDADE",
    "NOME_CIDADE",
    "VAR01",
    "VAR03",
    "VAR23",
    "VAR29",
    "VAR85",
    "VAR89"
  ) |>
  dplyr::rename(
    "populacao" = "VAR01",
    "pib" = "VAR03",
    "cige" = "VAR23",
    "cgp" = "VAR29"
  ) |>
  dplyr::mutate(
    "populacao" = as.numeric(populacao),
    "pib" = as.numeric(pib),
    "cige" = as.numeric(cige),
    "cgp" = as.numeric(cgp),
    "banco_publico" = ifelse(VAR85 | VAR89, 1, 0),
    "log_cige" = ifelse(as.numeric(cige) < 1, 0, log(as.numeric(cige))),
    "log_cgp" = ifelse(as.numeric(cgp) < 1, 0, log(as.numeric(cgp)))
  )

df_regic <- na.omit(df_regic)

df_regic$pib_pc <- df_regic$pib / df_regic$populacao
```

```{r echo=FALSE}

palette1 <- c("#264653", "#2a9d8f", "#e9c46a", "#f4a261","#e76f51")

municipios <- geobr::read_municipality(code_muni = 31, showProgress = FALSE)

municipios <- municipios |> dplyr::left_join(
  df_regic,
  by = c("code_muni" = "COD_CIDADE")
)

pib_map <- ggplot2::ggplot(municipios) +
  ggplot2::geom_sf(mapping = aes(fill = pib_pc), lwd = 0, colour = NA) +
  ggplot2::scale_fill_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "PIB per capita",
    na.value = "grey80",
  ) +
  theme_void() +
  theme(
    text = element_text(face = "plain"),
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12, face = "plain"),
  ) +
  annotation_scale(location = "bl", line_width = 0.4) +
  annotation_north_arrow(
    location = "br",
    style = north_arrow_fancy_orienteering(),
    height = unit(1, "cm"),
    width = unit(1, "cm")
  )

ggsave("exports/pibmap.pdf", plot = pib_map, width = 5000, height = 5000, units = "px", dpi = 320)

cige_map <- ggplot2::ggplot(municipios) +
  ggplot2::geom_sf(mapping = aes(fill = cige), lwd = 0, colour = NA) +
  ggplot2::scale_fill_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "CIGE",
    na.value = "grey80",
  ) +
  theme_void() +
  theme(
    text = element_text(face = "plain"),
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12, face = "plain"),
  ) +
  annotation_scale(location = "bl", line_width = 0.4) +
  annotation_north_arrow(
    location = "br",
    style = north_arrow_fancy_orienteering(),
    height = unit(1, "cm"),
    width = unit(1, "cm")
  )

ggsave("exports/cigemap.pdf", plot = cige_map, width = 5000, height = 5000, units = "px", dpi = 320)

cgp_map <- ggplot2::ggplot(municipios) +
  ggplot2::geom_sf(mapping = aes(fill = cgp), lwd = 0, colour = NA) +
  ggplot2::scale_fill_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "CGP",
    na.value = "grey80",
  ) +
  theme_void() +
  theme(
    text = element_text(face = "plain"),
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12, face = "plain"),
  ) +
  annotation_scale(location = "bl", line_width = 0.4) +
  annotation_north_arrow(
    location = "br",
    style = north_arrow_fancy_orienteering(),
    height = unit(1, "cm"),
    width = unit(1, "cm")
  )

ggsave("exports/cgpmap.pdf", plot = cgp_map, width = 5000, height = 5000, units = "px", dpi = 320)
```

Foi necessário excluir os registros que tinham alguma variável de interesse NULA (NA). Optou-se por não substituir os *NA* por valores arbitrários, como zero ou qualquer outro valor, pois causaria um grande viés na regressão, visto que se tratam municípios que não tiverem essas variáveis mensuradas.

## Análise Descritiva

Nesta seção, uma análise descritiva abrangente das variáveis em questão será conduzida, visando fornecer uma compreensão profunda de seus padrões, distribuições e relações. Esta abordagem permite não apenas a caracterização detalhada de cada variável individualmente, mas também a identificação de tendências e padrões globais dentro do conjunto de dados.

A Tabela @tbl-sum-vars contém as principais estatísticas descritivas das variáveis analisadas (PIB per capita, Centralidade da Gestão Pública (CGP) e Coeficiente de Intensidade da Gestão Empresarial (CIGE)) para as 168 observações (municípios) restantes na base:
```{r, results='asis', echo=FALSE}
#| label: tbl-sum-vars
#| tbl-cap: Estatísticas descritivas das variáveis
summary_vars <- data.frame(as.list(summary(df_regic$cige))) |>
  rbind(data.frame(as.list(summary(df_regic$cgp)))) |>
  rbind(data.frame(as.list(summary(df_regic$pib_pc))))

summary_vars <- cbind(c("CIGE", "CGP", "PIBpc"), summary_vars)

names(summary_vars) <- c("Variável", names(summary(df_regic$cige)))

stargazer::stargazer(
  summary_vars,
  header=FALSE,
  type='latex',
  summary = FALSE,
  rownames = FALSE,
  style = "aer",
  notes = "Fonte: Elaboração própria.",
  digits = 2,
  font.size = "small",
  no.space = TRUE
)
```

Abaixo, plotamos os histogramas dessas variáveis:
```{r, echo=FALSE}
#| fig-cap: Histograma do PIB per capita dos municípios de Minas Gerais (2018)

ggplot2::ggplot(df_regic, aes(x = pib_pc)) +
  ggplot2::geom_histogram(
    aes(y = ..density..),
    fill = "#00b4ef",
    color = "#ffffff",
  ) +
  ggplot2::geom_density(color="#bf80ff", size=0.5) +
  ggplot2::ylim(0, 0.075) +
  ggplot2::labs(
    x = "PIB per capita",
    y = "Densidade",
  ) +
  ggplot2::theme_classic() +
  ggplot2::theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(size = 0.3),
    title = element_text(size = 8, colour = "#222222"),
    plot.title = element_text(hjust = 0.5)
  )
```

```{r, echo=FALSE}
#| fig-cap: Histograma do Coeficiente de Intensidade da Gestão Empresarial (CIGE) dos municípios de Minas Gerais (2018)
ggplot2::ggplot(df_regic, aes(x = cige)) +
  ggplot2::geom_histogram(
    aes(y = ..density..),
    fill = "#00b4ef",
    color = "#ffffff",
  ) +
  ggplot2::geom_density(color="#bf80ff", size=0.5) +
  ggplot2::labs(
    x = "CIGE",
    y = "Densidade",
  ) +
  ggplot2::theme_classic() +
  ggplot2::theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(size = 0.3),
    title = element_text(size = 8, colour = "#222222"),
    plot.title = element_text(hjust = 0.5)
  )
```

```{r, echo=FALSE}
#| fig-cap: Histograma da Centralidade da Gestão Pública (CGP) dos municípios de Minas Gerais (2018)

ggplot2::ggplot(df_regic, aes(x = cgp)) +
  ggplot2::geom_histogram(
    aes(y = ..density..),
    fill = "#00b4ef",
    color = "#ffffff",
  ) +
  ggplot2::geom_density(color="#bf80ff", size=0.5) +
  ggplot2::labs(
    x = "CGP",
    y = "Densidade",
  ) +
  ggplot2::theme_classic() +
  ggplot2::theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(size = 0.3),
    title = element_text(size = 8, colour = "#222222"),
    plot.title = element_text(hjust = 0.5)
  )
```

Podemos ver a correlação entre as variáveis por meio de um mapa de calor obtido da matriz de correlação:
```{r, results='asis'}
#| label: fig-cor-heatmap
#| fig-cap: "Mapa de calor da matriz de correlação."

# Matriz com as variáveis [Y|X]
matrix <- df_regic |>
  dplyr::select("pib_pc", "cige", "cgp") |>
  as.matrix.data.frame()


matrix |> cor() |> heatmap()
```

Outra medida importante é a de covariância entre as variáveis, descrita na matriz de covariância abaixo (variância na diagonal principal):

```{r}
matrix |> cov() |> round(2)
```

Abaixo plotamos os *boxplots* das variáveis utilizadas:
```{r}
#| label: fig-box-cgp
#| fig-cap: "Boxplot da Centralidade da Gestão Pública (CGP) dos municípios de Minas Gerais (2018)"
boxplot(df_regic$cgp)
```

```{r}
#| label: fig-box-cige
#| fig-cap: "Boxplot do Coeficiente de Intensidade da Gestão Empresarial (CIGE) dos municípios de Minas Gerais (2018)"
boxplot(df_regic$cige)
```

```{r}
#| label: fig-box-pib
#| fig-cap: "Boxplot do PIB per capita dos municípios de Minas Gerais (2018)"
boxplot(df_regic$cige)
```

A visualização geográfica das variáveis do modelo (para o caso de municípios) é essencial para compreender, com clareza, onde os processos estão acontecendo:

\fig{PIB per capita dos municípios de Minas Gerais - REGIC 2018}{fig:pibpc}{exports/pibmap.pdf}{Elaboração própria.}

\fig{CIGE dos municípios de Minas Gerais - REGIC 2018}{fig:cige}{exports/cigemap.pdf}{Elaboração própria.}

\fig{CGP dos municípios de Minas Gerais - REGIC 2018}{fig:cgp}{exports/cgpmap.pdf}{Elaboração própria.}

Por fim, podemos plotar alguns gráficos de dispersão para esboçar a relação entre as variáveis do modelo:

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-cige-scatter
#| fig-cap: "PIB per capita x CIGE (Municípios de Minas Gerais - REGIC 2018)"

muni <- municipios[order(-municipios$pib_pc), ]
top3 <- muni$name_muni[1:3]

muni <- municipios[order(municipios$pib_pc), ]
low3 <- muni$name_muni[1:3]

ggplot2::ggplot(municipios,  aes(x = log(cige), y = log(pib_pc), label = name_muni)) +
  ggplot2::geom_point(mapping = aes(colour = pib_pc)) +
  ggplot2::scale_color_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "PIB per capita",
  ) +
  ggplot2::labs(
    x = "Log do CIGE",
    y = "Log do PIB per capita",
  ) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-cgp-scatter
#| fig-cap: "PIB per capita x CGP (Municípios de Minas Gerais - REGIC 2018)"

muni <- municipios[order(-municipios$pib_pc), ]
top3 <- muni$name_muni[1:3]

muni <- municipios[order(municipios$pib_pc), ]
low3 <- muni$name_muni[1:3]

ggplot2::ggplot(municipios,  aes(x = log(cgp), y = log(pib_pc), label = name_muni)) +
  ggplot2::geom_point(mapping = aes(colour = pib_pc)) +
  ggplot2::scale_color_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "PIB per capita",
  ) +
  ggplot2::labs(
    x = "Log do CGP",
    y = "Log do PIB per capita",
  ) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

## Análise de Regressão

Com as mesmas operações definidas na seção sobre Regressão Por Mínimos Quadrados, vamos estimar um modelo linear que tenta explicar o $ln(PIBpc)$ (logaritmo natural do PIB per capita) de municípios mineiros, com variáveis que medem intensidade de gestão empresarial (CIGE) e nível de centralidade da gestão pública (CGP).

```{r}
# Regressão simples
covxy <- cov(df_regic$log_cige, log(df_regic$pib_pc))
varx <- var(df_regic$log_cige)
mediay <- mean(log(df_regic$pib_pc))
mediax <- mean(df_regic$log_cige)
b1 <- covxy/varx

b0 <- mediay - b1*mediax

print(paste0("Intercepto: ", round(b0, 2)))
print(paste0("Coeficiente estimado: ", round(b1, 2)))
```

Com esses coeficientes já é possível traçar uma reta de regressão no gráfico:

```{r, echo=FALSE}
ggplot2::ggplot(municipios,  aes(x = log(cige), y = log(pib_pc), label = name_muni)) +
  ggplot2::geom_point(mapping = aes(colour = pib_pc)) +
  ggplot2::geom_abline(intercept = b0, slope = b1, color = "red") +
  ggplot2::scale_color_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "PIB per capita",
  ) +
  ggplot2::labs(
    x = "Log do CIGE",
    y = "Log do PIB per capita",
  ) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

Para verificar o ajuste do modelo, podemos calcular o $R^2$, dado que $R^2 = Var(\hat{y})/Var(y)$:

```{r}
# Cálculo do R-quadrado
y <- log(df_regic$pib_pc)
x <- log(df_regic$cige)
y.hat <- b0 + b1*x
r2 <- var(y.hat)/var(y)
r2
```

Esse resultado indica que cerca de $16,10\%$ da variação do log do PIB per capita dos municípios da amostra é explicado pelo log do Coeficiente de Interação da Gestão Empresarial (CIGE) desses municípios.

## Análise de Regressão Múltipla

Vamos acrescentar mais uma variável no modelo anterior e estimar novamente os parâmetros por meio de álgebra matricial:

```{r}
# Número de observações da amostra
n <- nrow(df_regic)

# Número de variáveis independentes
k <- 2

# Matrix de variáveis independentes
X <- matrix(1, nrow = n, ncol = 3) # Coluna de 1 (intercepto)
X[ ,2] <- log(df_regic$cige) # CIGE
X[ ,3] <- log(df_regic$cgp) # CGP

# Vetor de variável observada
Y <- as.matrix(log(df_regic$pib_pc))

# Vetor de parâmetros estimados
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y

# Vetor de resíduos estimados
uhat <- y - X %*% bhat

# Variância dos resíduos
sigma2hat <- as.numeric (t(uhat) %*% uhat / (nrow(df_regic)-k-1))

# Matriz var-cov dos coeficientes
varbetahat <- sigma2hat * solve(t(X) %*% X)
erropadraobeta <- sqrt (diag(varbetahat))

bhat
sigma2hat
varbetahat
erropadraobeta
```

Com apenas 2 variáveis independentes ainda é possível ver graficamente essa regressão, e agora, ao invés de uma reta de regressão, temos um plano de regressão:

```{r, echo=FALSE, warning=FALSE}
mqo <- lm(log(pib_pc) ~ log(cige) + log(cgp), df_regic)
grafico <- scatterplot3d(
  log(df_regic$cige), log(df_regic$cgp), log(df_regic$pib_pc),
  main = "Regressão Múltipla",
  xlab = "CIGE",
  ylab = "CGP",
  zlab = "PIB per capita",
  color = "blue"
)

y.hat <- X %*% bhat

grafico$plane3d(mqo, col = "red", alpha = 0.5, draw_polygon = T, draw_lines = T)
```

Como fizemos para a regressão simples, podemos calcular o ajuste $R^2$ para o modelo de regressão múltipla:

```{r}
r2 <- var(y.hat)/var(y)
r2
```

Esse resultado indica que cerca de $21,55\%$ da variação do log do PIB per capita dos
municípios da amostra é explicada pela combinação linear entre o log do Coeficiente de Interação da Gestão Empresarial(CIGE) e o log do nível de Centralidade da Gestão Pública (CGP) desses municípios.

## Análise de Regressão com diferentes formas funcionais

Optamos por substituir as variáveis utilizadas anteriormente para aumentar o tamanho da amostra e inferir melhor sobre os impactos de variáveis institucionais sobre os municípios mineiros. No lugar do Índice de Centralidade da Gestão Pública, incluímos uma varíavel de valor agregado da administração pública, que está completa na REGIC 2018. Além disso, trocamos o coeficiente de Gestão Empresarial pelo Índice de Atração Geral, que também está completo. Essas alterações mudaram o tamanho da amostra de 168 municípios (arranjos populacionais) para 770.

Para melhorar a análise sobre a presença de bancos nos munípios, adicionamos variáveis de uma base geolocalizada da ESTBAN para completar os dados faltantes na REGIC 2018.

Estimamos 2 modelos, incorporando diferentes formas funcionais. Ambos linearizam as variáveis contínuas com características exponenciais por meio de logaritmo. O primeiro, utiliza dummies da presença de bancos públicos e privados no munícipio, dando relevância à presença de bancos na estimação do PIB per capita. O segundo, utiliza variáveis que mensuram a distância do centróide de um munícipio até o centróide do município com agência bancária (separando por público e privado) mais próximo (municípios com agências bancárias recebem valor 0).


```{r}
df_regic2 <- readxl::read_xlsx(
  path = "data/REGIC2018 Cidades v2.xlsx",
  sheet="Base de dados por Cidades"
) |>
  dplyr::filter(
    UF == "MG"
  ) |>
  dplyr::select(
    "COD_CIDADE",
    "NOME_CIDADE",
    "VAR01",
    "VAR03",
    "VAR07",
    "VAR56",
    "VAR80",
    "VAR82",
    "VAR01"
  ) |>
  dplyr::rename(
    "populacao" = "VAR01",
    "pib" = "VAR03",
    "va_adm_publica" = "VAR07",
    "ia" = "VAR56"
  ) |>
  dplyr::mutate(
    "populacao" = as.numeric(populacao),
    "pib" = as.numeric(pib),
    "va_adm_publica" = as.numeric(va_adm_publica),
    "log_ia" = ifelse(as.numeric(ia) < 1, 0, log(as.numeric(ia)))
  )

df_regic2 <- na.omit(df_regic2)

df_regic2$pib_pc <- df_regic2$pib / df_regic2$populacao


project_id <- "cloud-learning-doing"

sql <- "SELECT * FROM estban.estban_agencias_geolocalizadas WHERE data_base = '2018-12-01'"

query <- bigrquery::bq_project_query(
  project_id,
  sql,
)

agencias_2018 <- bigrquery::bq_table_download(query)

minas_gerais <- geobr::read_state(code_state = 31, year = 2010, showProgress = FALSE, simplified = FALSE)

# Agências bancárias dentro do território mineiro
agencias_2018_mg <- agencias_2018 |>
  dplyr::filter(!is.na(lat) & !is.na(lng)) |>
  sf::st_as_sf(coords = c("lng", "lat"), crs = 4674) |>
  sf::st_filter(minas_gerais) |>
  dplyr::filter(uf == "MG")

# CNPJ dos bancos públicos (BB e CEF)
cnpj_banco_publico <- c("00000000", "00360305")

# Classificando as agências
agencias_2018_mg <- agencias_2018_mg |> mutate(
  tipo_banco = case_when(
    cnpj %in% cnpj_banco_publico ~ "Banco público",
    TRUE ~ "Banco privado"
  )
)

# Enriquecendo municípios com os dados da REGIC

municipios <- geobr::read_municipality(code_muni = 31, showProgress = FALSE, simplified = FALSE)

municipios <- municipios |> dplyr::left_join(
  df_regic2,
  by = c("code_muni" = "COD_CIDADE")
)

# Contando os bancos públicos e privados por cidade
bancos_municipios <- agencias_2018_mg |>
  sf::st_drop_geometry() |>
  dplyr::group_by(cod_mun_ibge, tipo_banco) |>
  dplyr::summarise(agencias = n()) |>
  tidyr::pivot_wider(
        names_from = tipo_banco,
        values_from = agencias,
        values_fill = 0
  ) |>
  dplyr::mutate(
    "cod_mun_ibge" = as.numeric(cod_mun_ibge),
    "banco_publico" = ifelse(`Banco público` > 0, 1, 0),
    "banco_privado" = ifelse(`Banco privado` > 0, 1, 0)
  ) |>
  dplyr::select(
    "cod_mun_ibge",
    "banco_publico",
    "banco_privado"
  ) # Dummies para presença de banco público e/ou banco privado

# Enriquecendo com as informações sobre banco
municipios <- municipios |> dplyr::left_join(
  bancos_municipios,
  by = c("code_muni" = "cod_mun_ibge")
) |> dplyr::mutate(
  "banco_publico" = ifelse(is.na(banco_publico), 0, banco_publico),
  "banco_privado" = ifelse(is.na(banco_privado), 0, banco_privado)
)

# Adicionando os centróidos dos municípios
municipios <- sf::st_make_valid(municipios)
municipios$centroid <- sf::st_centroid(municipios)

coords <- sf::st_coordinates(municipios$centroid)
mat.dist <- as.matrix(dist(coords, method = "euclidean"))
dist2 <- mat.dist^2
diag(dist2) <- 0

rownames(dist2) <- municipios$code_muni
colnames(dist2) <- municipios$code_muni

municipios_banco_publico <- municipios |> dplyr::filter(banco_publico == 1)

municipios_banco_privado <- municipios |> dplyr::filter(banco_privado == 1)

resultados <- data.frame(city = character(nrow(dist2)), dist2.min.banco.publico = numeric(nrow(dist2)), dist2.min.banco.privado = numeric(nrow(dist2)))

for(i in 1:nrow(dist2)){
  city <- names(dist2[, 1])[i]

  if (city %in% as.character(municipios_banco_publico$code_muni)){
    dist.pub <- 0
  } else {
    line <- dist2[i,]
    dist.pub <- 100000000000
    for (j in 1:length(line)) {
      if (j != i & line[j] < dist.pub){
        if (names(line)[j] %in% as.character(municipios_banco_publico$code_muni))
          dist.pub <- line[j]
      }
    }
  }

  if (city %in% as.character(municipios_banco_privado$code_muni)){
    dist.priv <- 0
  } else {
    line <- dist2[i,]
    dist.priv <- 100000000000
    for (j in 1:length(line)) {
      if (j != i & line[j] < dist.priv){
        if (names(line)[j] %in% as.character(municipios_banco_privado$code_muni))
          dist.priv <- line[j]
      }
    }
  }

  resultados[i, 1] <- city
  resultados[i, 2] <- dist.pub
  resultados[i, 3] <- dist.priv
}

resultados <- resultados |> mutate("cod" = as.numeric(city))

municipios <- municipios |> dplyr::left_join(
  resultados,
  by = c("code_muni" = "cod")
)

municipios <- na.omit(municipios)

modelo_dummies <- lm(log(pib_pc) ~ log_ia + log(va_adm_publica) + banco_publico + banco_privado, municipios)

modelo_dist <- lm(log(pib_pc) ~ log_ia + log(va_adm_publica) + dist2.min.banco.publico + dist2.min.banco.privado, municipios)
```

A @fig-va mostra como o valor adicionado da Administração Pública está correlacionada com o PIB per capita, controlando pelos tipos de bancos presentes nos munícipios. As diferentes linhas de regressão mostram como a presença de um banco público eleva o nível de renda per capita dos municípios. O efeito dos bancos privados não foi significativo.Tal resultado pode estar ligado a fatores institucionais, os quais criam uma dependência de trajetória no tempo. A opção de estabelecer um banco público em determinado munícipio em um certo período de tempo fez com que se produzisse todo um arranjo institucional que possibilita a elevação dos rendimentos e, dado esse arranjo, ocorre uma dinamização em torno do mesmo, de modo que as características tendem a se perpetuar, como explica North (1990).

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-va
#| fig-cap: "PIB per capita x Valor adicionado da Administração Pública (Municípios de Minas Gerais - REGIC 2018)"

municipios$dummy_grupo <- factor(
  case_when(
    municipios$banco_publico == 1 & municipios$banco_privado == 1 ~ "Ambos",
    municipios$banco_publico == 1 ~ "Somente Público",
    municipios$banco_privado == 1 ~ "Somente Privado",
    TRUE ~ "Nenhum"
  )
)

muni <- municipios[order(-municipios$pib_pc), ]
top3 <- muni$name_muni[1:3]

muni <- municipios[order(municipios$pib_pc), ]
low3 <- muni$name_muni[1:3]

ggplot2::ggplot(municipios,  aes(x = log(va_adm_publica), y = log(pib_pc), label = name_muni, color = factor(dummy_grupo))) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(intercept = modelo_dummies$coefficients[1], slope = modelo_dummies$coefficients[3], color = "#f8766d") +
  ggplot2::geom_abline(intercept = modelo_dummies$coefficients[1] + modelo_dummies$coefficients[4], slope = modelo_dummies$coefficients[3], color = "#00bc59") +
  ggplot2::geom_abline(intercept = modelo_dummies$coefficients[1] + modelo_dummies$coefficients[4] + modelo_dummies$coefficients[5], slope = modelo_dummies$coefficients[3], color = "#00b4ef") +
  ggplot2::geom_abline(intercept = modelo_dummies$coefficients[1] + modelo_dummies$coefficients[5], slope = modelo_dummies$coefficients[3], color = "#bf80ff") +
  ggplot2::scale_color_manual(
    values = c("#00b4ef", "#00bc59", "#bf80ff", "#f8766d"),  # Adjust colors as needed
    labels = c("Ambos", "Somente Público", "Somente Privado", "Nenhum"),  # Update legend labels
    name = "Tipo Banco"  # Update legend title
  ) +
  ggplot2::labs(
    x = "Log do VA da administração pública",
    y = "Log do PIB per capita",
  ) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

A @fig-ia mostra que o Índice de Atratividade não está tão relacionado com PIB per capita. Apesar de ter um efeito estatísticamente diferente de zero, nem conseguimos ver as retas de regressão no gráfico, ou seja, nesse modelo em específico, não há tanto ajuste para essa variável. Os municípios com população pequena em Minas Gerais e que não são tão atrativos comercialmente, podem estar sobrevalorizando o PIB per capita e distorcendo a análise. A @fig-ia-pib mostra que o Índice de Atratividade está muito mais correlacionado ao tamanho do PIB, das grandes cidades.

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-ia
#| fig-cap: "PIB per capita x Índice de Atração Geral (Municípios de Minas Gerais - REGIC 2018)"

municipios$dummy_grupo <- factor(
  case_when(
    municipios$banco_publico == 1 & municipios$banco_privado == 1 ~ "Ambos",
    municipios$banco_publico == 1 ~ "Somente Público",
    municipios$banco_privado == 1 ~ "Somente Privado",
    TRUE ~ "Nenhum"
  )
)

muni <- municipios[order(-municipios$pib_pc), ]
top3 <- muni$name_muni[1:3]

muni <- municipios[order(municipios$pib_pc), ]
low3 <- muni$name_muni[1:3]

ggplot2::ggplot(municipios,  aes(x = log_ia, y = log(pib_pc), label = name_muni, color = factor(dummy_grupo))) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(intercept = modelo_dummies$coefficients[1], slope = modelo_dummies$coefficients[2], color = "#f8766d") +
  ggplot2::geom_abline(intercept = modelo_dummies$coefficients[1] + modelo_dummies$coefficients[4], slope = modelo_dummies$coefficients[2], color = "#00bc59") +
  ggplot2::geom_abline(intercept = modelo_dummies$coefficients[1] + modelo_dummies$coefficients[4] + modelo_dummies$coefficients[5], slope = modelo_dummies$coefficients[2], color = "#00b4ef") +
  ggplot2::geom_abline(intercept = modelo_dummies$coefficients[1] + modelo_dummies$coefficients[5], slope = modelo_dummies$coefficients[2], color = "#bf80ff") +
  ggplot2::scale_color_manual(
    values = c("#00b4ef", "#00bc59", "#bf80ff", "#f8766d"),  # Adjust colors as needed
    labels = c("Ambos", "Somente Público", "Somente Privado", "Nenhum"),  # Update legend labels
    name = "Tipo Banco"  # Update legend title
  ) +
  ggplot2::labs(
    x = "Log do Índice de Atração",
    y = "Log do PIB per capita",
  ) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-ia-pib
#| fig-cap: "PIB x Índice de Atração Geral (Municípios de Minas Gerais - REGIC 2018)"

municipios$dummy_grupo <- factor(
  case_when(
    municipios$banco_publico == 1 & municipios$banco_privado == 1 ~ "Ambos",
    municipios$banco_publico == 1 ~ "Somente Público",
    municipios$banco_privado == 1 ~ "Somente Privado",
    TRUE ~ "Nenhum"
  )
)

muni <- municipios[order(-municipios$pib), ]
top3 <- muni$name_muni[1:3]

muni <- municipios[order(municipios$pib), ]
low3 <- muni$name_muni[1:3]

model <- lm(log(pib) ~ log_ia + banco_publico + banco_privado, municipios)

ggplot2::ggplot(municipios,  aes(x = log_ia, y = log(pib), label = name_muni, color = factor(dummy_grupo))) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2], color = "#f8766d") +
  ggplot2::geom_abline(intercept = model$coefficients[1] + modelo_dummies$coefficients[3], slope = model$coefficients[2], color = "#00bc59") +
  ggplot2::geom_abline(intercept = model$coefficients[1] + model$coefficients[3] + model$coefficients[4], slope = model$coefficients[2], color = "#00b4ef") +
  ggplot2::geom_abline(intercept = model$coefficients[1] + model$coefficients[4], slope = model$coefficients[2], color = "#bf80ff") +
  ggplot2::scale_color_manual(
    values = c("#00b4ef", "#00bc59", "#bf80ff", "#f8766d"),  # Adjust colors as needed
    labels = c("Ambos", "Somente Público", "Somente Privado", "Nenhum"),  # Update legend labels
    name = "Tipo Banco"  # Update legend title
  ) +
  ggplot2::labs(
    x = "Log do Índice de Atração",
    y = "Log do PIB",
  ) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

A @fig-dist-pub mostra que quanto mais distantes os municípios estão de um banco público, menos renda per capita eles detêm, indicando menor desenvolvimento à medida que se dificulta o acesso às agências, dado que há maiores custos de deslocamento e menor interação entre agentes mais distantes no espaço. Tal resultado é amplamente discutido no âmbito da literatura regional, reforçando a Teoria do Lugar Central, proposta por Christaller (1966). Supondo que essas regiões que possuem bancos públicos exerçam uma espécie de centralidade, elas geram economias de localização e urbanização, as quais se traduzem em spillovers para regiões próximas, beneficiando seu crescimento econômico.

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-dist-pub
#| fig-cap: "PIB per capita x Distância ao quadrado da agência (banco público) mais próxima (Municípios de Minas Gerais - REGIC 2018)"

muni <- municipios[order(-municipios$dist2.min.banco.publico), ]
top3 <- muni$name_muni[1:3]

muni <- municipios[order(-municipios$pib_pc), ]
low3 <- muni$name_muni[1:3]

model <- lm(log(pib_pc) ~ dist2.min.banco.publico, municipios)

ggplot2::ggplot(municipios,  aes(x = dist2.min.banco.publico, y = log(pib_pc), label = name_muni)) +
  ggplot2::geom_point(mapping = aes(colour = pib_pc)) +
  ggplot2::geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2], color = "red") +
  ggplot2::scale_color_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "PIB per capita",
  ) +
  ggplot2::labs(
    x = "Distância ao quadrado para banco público",
    y = "Log do PIB",
  ) +
  ggplot2::xlim(0, 0.6) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

O mesmo acontece para a distância que os municípios estão de bancos privados (@fig-dist-priv), mas numa intensidade menor. No modelo com ambas as variáveis, apenas a distância para o banco público foi significativa na estimação. Uma hipótese para tal resultado é a ampla dispersão de bancos privados nos munícipios, mesmo aqueles de menor tamanho, o que não ocorre com os bancos públicos.

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-dist-priv
#| fig-cap: "PIB per capita x Distância ao quadrado da agência (banco privado) mais próxima (Municípios de Minas Gerais - REGIC 2018)"

muni <- municipios[order(-municipios$dist2.min.banco.privado), ]
top3 <- muni$name_muni[1:3]

muni <- municipios[order(-municipios$pib_pc), ]
low3 <- muni$name_muni[1:3]

model <- lm(log(pib_pc) ~ dist2.min.banco.privado, municipios)

ggplot2::ggplot(municipios,  aes(x = dist2.min.banco.privado, y = log(pib_pc), label = name_muni)) +
  ggplot2::geom_point(mapping = aes(colour = pib_pc)) +
  ggplot2::geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2], color = "red") +
  ggplot2::scale_color_gradientn(
    colors = palette1,
    trans = "log1p",
    name = "PIB per capita",
  ) +
  ggplot2::labs(
    x = "Distância ao quadrado para banco privado",
    y = "Log do PIB",
  ) +
  ggplot2::xlim(0, 0.6) +
  ggrepel::geom_text_repel(
    data = subset(
      municipios,
      name_muni %in% c(top3, low3)
    ),
    box.padding = 0.5,
    size = 3
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

A @tbl-model mostra o resultado da estimação e serve de comparativo entre os modelos com dummy bancária e o que incorpora o quadrado da distância.

```{r echo=FALSE, results='asis', warning=FALSE}
#| label: tbl-model
#| tbl-cap: Modelos estimados

stargazer::stargazer(
  modelo_dummies,
  modelo_dist,
  header=FALSE,
  type='latex',
  summary = FALSE,
  rownames = FALSE,
  style = "aer",
  notes = "Fonte: Elaboração própria.",
  dep.var.labels = c("Dummies", "Distância"),
  digits = 3,
  font.size = "small",
  no.space = TRUE
)
```

## Testes

Para testar multicolinearidade, podemos utilizar o fator de inflacionamento da variância (VIF), definido na seção sobre multicolinearidade:

```{r}
library(car)

# Modelo de dummies
car::vif(modelo_dummies)
```

```{r}
# Modelo de distância
car::vif(modelo_dist)
```

Como os valores do VIF são menores do que 4, podemos dizer que não há multicolinearidade prejudicial em nenhum dos modelos estimados. Não sendo necessário realizar ajustes nesse sentido.

No que tange a heterocedasticidade, podemos tentar visualizar graficamente a interação entre os resíduos e os valores estimados:

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-residuos-dummies
#| fig-cap: "PIB per capita estimado x Resíduos (Modelo com dummies bancárias)"


model <- lm(modelo_dummies$residuals ~ modelo_dummies$fitted.values)

ggplot2::ggplot() +
  ggplot2::geom_point(mapping = aes(x = modelo_dummies$fitted.values, y = modelo_dummies$residuals, colour = modelo_dummies$residuals**2)) +
  ggplot2::geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2], color = "red") +
  ggplot2::scale_color_gradientn(
    colours = palette1,
    name = "û²"
  ) +
  ggplot2::ylim(-2, 2) +
  ggplot2::labs(
    x = "PIB per capita estimado",
    y = "Resíduos",
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-residuos-dist
#| fig-cap: "PIB per capita estimado x Resíduos (Modelo com distância)"


model <- lm(modelo_dist$residuals ~ modelo_dist$fitted.values)

ggplot2::ggplot() +
  ggplot2::geom_point(mapping = aes(x = modelo_dist$fitted.values, y = modelo_dist$residuals, colour = modelo_dist$residuals**2)) +
  ggplot2::geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2], color = "red") +
  ggplot2::scale_color_gradientn(
    colours = palette1,
    name = "û²"
  ) +
  ggplot2::ylim(-2, 2) +
  ggplot2::labs(
    x = "PIB per capita estimado",
    y = "Resíduos",
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

A princípio, graficamente, não há correlação entre os resíduos e os valores estimados, indicando que, possivelmente, não há heterocedasticidade nos modelos estimados. Entretanto, uma análise gráfica não é o melhor método para detectar este problema e, portanto, faremos o teste de Breusch-Pagan para verificar se os erros são normalmente distribuídos. A hipótese nula é de que a variância dos resíduos é igual à $\sigma^2$. Rejeitaremos a hipótese nula, caso $p-value < 0,05$. Em outras palavras, se o $p-value$ é menor que o nível de significância de 5%, a chance de estarmos cometendo o erro do tipo 1 é muito baixa, o que nos leva a aceitar a hipótese alternativa, i.e., de que os erros são heterocedásticos. O teste está realizado abaixo:

```{r}
library(lmtest)

lmtest::bptest(modelo_dummies)
```

```{r}
lmtest::bptest(modelo_dummies)
```

Como em ambos os modelos, $p-value < 0,05$ rejeitamos a hipótese nula e tomamos os modelos como heterocedásticos.

Na presença de heterocedasticidade, os erros padrões não são confiáveis, e portanto devemos fazer a correção pela matriz de White para que incluir erros robustos aos modelos. A @fig-residuos-robustos-dummies e a @fig-residuos-robustos-dist mostram os erros após o ajuste.

```{r}
library(sandwich)

# Erros padrões robustos após correção pela matrix de White
modelo_dummies_robusto <- modelo_dummies
matriz_white <- as.matrix.data.frame(sandwich::vcovHC(modelo_dummies, type="HC0"))
erros_robustos <- sqrt(diag(matriz_white)) * modelo_dummies$residuals
modelo_dummies_robusto$residuals <- erros_robustos
```

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-residuos-robustos-dummies
#| fig-cap: "PIB per capita estimado x Resíduos Robustos (Modelo com dummies bancárias)"


model <- lm(modelo_dummies_robusto$residuals ~ modelo_dummies_robusto$fitted.values)

ggplot2::ggplot() +
  ggplot2::geom_point(mapping = aes(x = modelo_dummies_robusto$fitted.values, y = modelo_dummies_robusto$residuals, colour = modelo_dummies_robusto$residuals**2)) +
  ggplot2::geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2], color = "red") +
  ggplot2::scale_color_gradientn(
    colours = palette1,
    name = "û²"
  ) +
  ggplot2::ylim(-2, 2) +
  ggplot2::labs(
    x = "PIB per capita estimado",
    y = "Resíduos",
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

```{r}
# Erros padrões robustos após correção pela matrix de White
modelo_dist_robusto <- modelo_dist
matriz_white <- as.matrix.data.frame(sandwich::vcovHC(modelo_dist, type="HC0"))
erros_robustos <- sqrt(diag(matriz_white)) * modelo_dist$residuals
modelo_dist_robusto$residuals <- erros_robustos
```

```{r, echo=FALSE, message=FALSE, result='asis'}
#| label: fig-residuos-robustos-dist
#| fig-cap: "PIB per capita estimado x Resíduos Robustos (Modelo com distância)"


model <- lm(modelo_dist_robusto$residuals ~ modelo_dist_robusto$fitted.values)

ggplot2::ggplot() +
  ggplot2::geom_point(mapping = aes(x = modelo_dist_robusto$fitted.values, y = modelo_dist_robusto$residuals, colour = modelo_dist_robusto$residuals**2)) +
  ggplot2::geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2], color = "red") +
  ggplot2::scale_color_gradientn(
    colours = palette1,
    name = "û²"
  ) +
  ggplot2::ylim(-2, 2) +
  ggplot2::labs(
    x = "PIB per capita estimado",
    y = "Resíduos",
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    text = ggplot2::element_text(size = 10, face = "bold"),
    legend.text = ggplot2::element_text(size = 10, face = "plain")
  )
```

A @tbl-model-robust mostra a diferença dos coeficientes após as correções de heterocedasticidade nos modelos.

```{r echo=FALSE, results='asis', warning=FALSE}
#| label: tbl-model-robust
#| tbl-cap: Comparação entre os modelos padrão e com erros robustos

stargazer::stargazer(
  modelo_dummies,
  modelo_dummies_robusto,
  modelo_dist,
  modelo_dist_robusto,
  header=FALSE,
  type='latex',
  summary = FALSE,
  rownames = FALSE,
  style = "aer",
  notes = "Fonte: Elaboração própria.",
  dep.var.labels = c("Dummies", "Dummies.R", "Dist", "Dist.R"),
  digits = 3,
  font.size = "small",
  no.space = TRUE
)
```

Observa-se, pela tabela 3, que em ambos os modelos o logarítimo do Indíce de Atração Financeira é significativos, no mínimo, ao nível de 5%. Já o logarítimo do Valor Adicionado da Admnistração Pública é, tmabém, significativo ao nível de 1%. Isso implica que essas variáveis estão influenciando positivamente o logarítimo do PIB per capita. Ademais, todos os modelos possuem uma constante significativa.

No modelo de dummies, por sua vez, temos que a presença de bancos públicos exerce uma influência positiva no PIB per capita do município, ao passo que a presença de bancos privados mostrou-se não significativa. Dessa forma, pode-se arguir que ter um banco público em um município promove uma elevação da renda deste mesmo, em comparação com os demais, onde não há presença do mesmo.

Por sua vez, o modelo com distâncias inversas aos quadrado, o qual visa captar centralidade obteve resultados que, de certa forma, corroboram os resultados encontrados no modelo com dummies. Isto porque só se mostraram significativos os coeficientes da distância inversa ao quadrado dos bancos públicos. Nesse sentido, quanto mais distante está um município de um banco público, maior tende a ser o efeito negativo em seu PIB per capita.

Dessa forma, após a análise dos modelos, conclui-se que os bancos públicos exercem uma importante influência no PIB per capita municipal, de modo que, além disso, fatores aglomerativos também parecem pesar nessa variável, haja vista que há indícios de centralidade de regiões que possuem bancos públicos em relação aos seus vizinhos, que não possuem. Exarceba-se, portanto, a influência das atividades financeiras para a elevação da renda dos munícipes mineiros.

## Teste de Hausman

Podemos fazer o teste de Durbin-Wu-Hausman para verificar endogeneidade no modelo.


```{r}
formula_estrutural <- log(pib_pc) ~ log_ia +
  log(va_adm_publica) +
  banco_publico +
  banco_privado

mqo_estrutural <- lm(formula_estrutural, municipios)
summary(mqo_estrutural)

# Utilizando índice de comércio como instrumento para índice de atração geral

z <- na.omit(municipios$VAR80)
x <- na.omit(municipios$log_ia)

cor(z, x)

formula_endogena <- log_ia ~ VAR80 +
  log(va_adm_publica) +
  banco_publico +
  banco_privado

mqo_endogeno <- lm(formula_endogena, municipios)
summary(mqo_endogeno)

residuos <- mqo_endogeno$residuals

teste <- lm(log(pib_pc) ~ log_ia +
  log(va_adm_publica) +
  banco_publico +
  banco_privado +
  residuos,
  municipios
)

summary(teste)
```

Como os resíduos foram significativos com $p < 0.05$, rejeitamos a hipótese nula de que a variável Índice de Atração é exógena. Logo, devemos prosseguir com o método de estimação MQ2E.

```{r}
# Estimação da variável instrumental
vi <- ivreg::ivreg(
  log(pib_pc) ~ log_ia +
  log(va_adm_publica) +
  banco_publico +
  banco_privado | VAR80 +
  log(va_adm_publica) +
  banco_publico +
  banco_privado,
  data=municipios 
)

summary(vi)
```

Confirmando o resultado obtido anteriormente, o teste de Wu-Hausman rejeitou a hipótese nula à 5% de significância, indicando que havia endogeneidade no modelo MQO. O teste de Weak, por sua vez, indicou que o instrumento adicionado é fortemente relacionado com a variável endógena.


```{r}
vi2 <- lm(
  vi$residuals ~ VAR80 +
  log(va_adm_publica) +
  banco_publico +
  banco_privado,
  data=municipios
)

summary(vi2)
```
Como podemos ver, os resíduos do MQ2E não são explicados pelas variáveis exógenas.


*Teste de sobreidentificação*

```{r}
n <- length(municipios$pib_pc)
q1 <- summary(vi2)$fstatistic[2] - summary(vi)$waldtest[3]

c(
  LM = summary(vi)$r.squared * n,
  pvalue = pchisq(summary(vi)$r.squared * n, df = q1, lower.tail = FALSE)
)
```

Como $p > 0.05$, aceitamos a hipótese nula do teste de sobreidentificação e assumimos que todas variáveis instrumentais são exógenas.

É comum que variáveis como PIB per capita, tenham relação endógena com outras variáveis econômicas, pois tanto variáveis como número de empresas numa cidade exercem influência sobre o PIB per capita, como o PIB per capita exerce influência sobre o número de empresas. Essa é uma relação circular, que é melhor estimada quando utilizamos séries temporais com defasagem nas variáveis explicativas, pois como tempo é unidirecional (do passado para o futuro), podemos captar, de fato, relações de causalidade.

A escolha dos instrumentos nesse trabalho foi extremamente difícil, pela natureza do problema e pela limitação da base de dados. Portanto, é de esperar que variáveis como "Valor adicionado da administração pública" também tenha relação endógena com PIB per capita.

A @fig-mqo-mqe compara os parâmetros entre os modelos MQO e MQ2E.

```{r}
#| fig-cap: "MQO x MQ2E"
#| label: fig-mqo-mqe

# Extrair coeficientes e erros padrão das equações
coef_eq1 <- summary(mqo_estrutural)$coefficients
coef_eq2 <- summary(vi)$coefficients

# Combinar coeficientes em um data frame
coef_table_eq1 <- data.frame(
  Equation = "MQO",
  Term = rownames(coef_eq1),
  Estimate = coef_eq1[, "Estimate"],
  StdError = coef_eq1[, "Std. Error"]
)

coef_table_eq2 <- data.frame(
  Equation = "MQ2E",
  Term = rownames(coef_eq2),
  Estimate = coef_eq2[, "Estimate"],
  StdError = coef_eq2[, "Std. Error"]
)

# Combinar os data frames das duas equações
coef_table <- rbind(coef_table_eq1, coef_table_eq2)

# Adicionar intervalos de confiança
coef_table$CI_Lower <- coef_table$Estimate - 1.96 * coef_table$StdError
coef_table$CI_Upper <- coef_table$Estimate + 1.96 * coef_table$StdError

ggplot(coef_table, aes(x = Term, y = Estimate, color = Equation)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(
       x = "Variável",
       y = "Estimativa",
       color = "Modelo") +
  theme_minimal()
```

## Modelo SUR

Como já descrito nas seções teóricas, o modelo SUR nos ajuda a estimar equações de forma simultânea, de modelos que podem parecer independentes, mas que podem ter correlação entre seus erros.

Na primeira equação, podemos relacionar o PIB per capita com o Índice de Atração Geral e a presenças dos bancos públicos e privados, para ver como os municípios são afetados por essas variáveis e estimar, numa segunda equação, o Índice de Atração Geral, pelo valor adicionado da administração público e pelo índice de comércio. A ideia é tentar separar os efeitos diretos (sobre o PIB) e indiretos (sobre o índice de atratividade) do valor adicionado da administração pública.

```{r}
eq1 <- log(pib_pc) ~ log_ia +
  log(va_adm_publica) +
  banco_publico +
  banco_privado

eq2 <- log_ia ~ log(va_adm_publica) +
  VAR80

sistema <- list(eq1, eq2)

# Estimando SUR
library(systemfit)

sur = systemfit::systemfit(sistema, "SUR", data = municipios)
summary(sur)
```

Como se pode observar, os resultados indicam uma correlação residual negativa e fraca, mas existente. Além disso, identificamos que o efeito direto do valor adicionado da administração pública sobre o PIB per capita é muito maior que o efeito indireto pela atratividade do município (@tbl-sur). As @fig-sur-params e @fig-sur-resid mostram a estimativa dos parâmetros e seus intervalos de confiança, e os resíduos, respectivamente.

```{r, results='asis', echo=FALSE}
#| label: tbl-sur
#| tbl-cap: "Modelo SUR"

texreg::texreg(sur,
  file = NULL,
  caption.above = TRUE,
  custom.note = "Fonte: Elaboração Própria"
)
```

```{r, echo=FALSE}
#| fig-cap: Parâmetros do modelo SUR
#| label: fig-sur-params

# Extrair coeficientes e erros padrão das equações
coef_eq1 <- summary(sur$eq[[1]])$coefficients
coef_eq2 <- summary(sur$eq[[2]])$coefficients

# Combinar coeficientes em um data frame
coef_table_eq1 <- data.frame(
  Equation = "Equação 1",
  Term = rownames(coef_eq1),
  Estimate = coef_eq1[, "Estimate"],
  StdError = coef_eq1[, "Std. Error"]
)

coef_table_eq2 <- data.frame(
  Equation = "Equação 2",
  Term = rownames(coef_eq2),
  Estimate = coef_eq2[, "Estimate"],
  StdError = coef_eq2[, "Std. Error"]
)

# Combinar os data frames das duas equações
coef_table <- rbind(coef_table_eq1, coef_table_eq2)

# Adicionar intervalos de confiança
coef_table$CI_Lower <- coef_table$Estimate - 1.96 * coef_table$StdError
coef_table$CI_Upper <- coef_table$Estimate + 1.96 * coef_table$StdError

ggplot(coef_table, aes(x = Term, y = Estimate, color = Equation)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(
       x = "Termo",
       y = "Estimativa",
       color = "Equação") +
  theme_minimal()
```

```{r, echo=FALSE}
#| fig-cap: Resíduos do modelo SUR
#| label: fig-sur-resid

# Obter resíduos das equações
residuals_eq1 <- residuals(sur$eq[[1]])
residuals_eq2 <- residuals(sur$eq[[2]])

# Combinar resíduos em um data frame
residuals_table <- data.frame(
  Equation = rep(c("Equação 1", "Equação 2"), each = length(residuals_eq1)),
  Fitted = c(fitted(sur$eq[[1]]), fitted(sur$eq[[2]])),
  Residuals = c(residuals_eq1, residuals_eq2)
)

# Criar gráfico de resíduos
ggplot(residuals_table, aes(x = Fitted, y = Residuals, color = Equation)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
       x = "Valores Ajustados",
       y = "Resíduos",
       color = "Equação") +
  theme_minimal()
```

Não podemos comparar os modelos SUR com o de MQO pelo $R^2$ ajustado, pois perdemos graus de liberdade ao utilizar mais variáveis na estimação. No entanto, podemos utilizar a estatística AIC para comparar todos os modelos feitos até aqui, pois o critério AIC visa encontrar o modelo que fornece o melhor ajuste aos dados com a menor complexidade possível:

```{r}
AIC(mqo_estrutural)
```

```{r}
AIC(sur)
```

```{r}
library(performance)

performance <- model_performance(vi, metrics = c("AIC"))

performance$AIC
```

Como o AIC do MQO é menor, podemos dizer que ele tem o melhor ajuste aos dados, ponderando pela sua baixa complexidade.

\newpage

# REFERÊNCIAS

\singlespacing

::: {#refs}
:::
